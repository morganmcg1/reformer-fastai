{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step by step walkthru of lucidrains [LSHSelfAttention module](https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**\n",
    "* Local attention from lucidrains is not directly relevant for the reformer\n",
    "* reused values per attention head is not standard afaik\n",
    "* post_attn_dropout is not standard to my knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, default_val):\n",
    "    return default_val if val is None else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_heads(v):\n",
    "    return v.view(b, kv_len, h, -1).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(v):\n",
    "    return v.view(b, h, t, -1).transpose(1, 2).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dims(ind_from, ind_to, tensor):\n",
    "    shape = list(tensor.shape)\n",
    "    arr_slice = slice(ind_from, ind_to + 1)\n",
    "    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n",
    "    return tensor.reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_at_index(dim, index, t):\n",
    "    pre_slices = (slice(None),) * dim\n",
    "    l = (*pre_slices, slice(None, index))\n",
    "    r = (*pre_slices, slice(index, None))\n",
    "    return t[l], t[r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs_chunk(fn, chunks=1, dim=0):\n",
    "    def inner_fn(*args, **kwargs):\n",
    "        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n",
    "        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n",
    "        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n",
    "        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n",
    "        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=128                          # note! This is `kqv` dim, not sl or embedding dim\n",
    "heads = 8\n",
    "bucket_size = 64\n",
    "n_hashes = 8\n",
    "causal = False\n",
    "dim_head = 64                     # set qk, v dim manually, otherwise calculated as dim/n_heads\n",
    "attn_chunks = 1                   # Provess attention calculation in chunks if we have memory concerns \n",
    "random_rotations_per_head = False # This is assumed to be false, and is removed in our LSHAttention implementation\n",
    "attend_across_buckets = False     # same as LSHAttention\n",
    "allow_duplicate_attention = False # same as LSHAttention\n",
    "num_mem_kv = 0                    # extra random paramteres added to x. Reason for this?\n",
    "one_value_head = False            # (True) one repeated v for all heads, or (False) separate value for each head (standard)\n",
    "use_full_attn = False             # Full attention or LSH\n",
    "full_attn_thres = None            # logic to decide if we can use full attention? \n",
    "return_attn = False               \n",
    "post_attn_dropout = 0.            # Dropout after attention is calculated\n",
    "dropout = 0.                      # Normal dropout passed to LSH layer\n",
    "n_local_attn_heads = 0            # disable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dim_head or (dim % heads) == 0, 'dimensions must be divisible by number of heads'\n",
    "assert n_local_attn_heads < heads, 'local attention heads must be less than number of heads'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set `dim_heads` to some custom value. If not set, `dim_heads = dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 512, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_head = default(dim_head, dim // heads)\n",
    "dim_heads = dim_head * heads\n",
    "dim_head, dim_heads, dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose to splut up attention caluclations in chunks to decrease peak memory (note: This is unlike LSH chunking. This trades comput time agains memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_chunks = default(attn_chunks, 1)\n",
    "attn_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `one_value_head` is True `v_head_repeats` is set to `n_heads`. That means that v is reused across attention heads. Not standard to my knowledge. Maybe save computations or memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_head_repeats = (heads if one_value_head else 1)\n",
    "v_dim = dim_heads // v_head_repeats\n",
    "attn_chunks, v_head_repeats, v_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up projection layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=128, out_features=512, bias=False),\n",
       " Linear(in_features=128, out_features=512, bias=False),\n",
       " Linear(in_features=512, out_features=128, bias=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toqk = nn.Linear(dim, dim_heads, bias = False)\n",
    "tov = nn.Linear(dim, v_dim, bias = False)\n",
    "to_out = nn.Linear(dim_heads, dim)\n",
    "toqk, tov, to_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout and attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reformer_pytorch import LSHAttention\n",
    "# self.bucket_size = bucket_size\n",
    "lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout)\n",
    "# self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n",
    "post_attn_dropout = nn.Dropout(post_attn_dropout)\n",
    "# self.use_full_attn = use_full_attn\n",
    "# self.full_attn_thres = default(full_attn_thres, bucket_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`num_mem_kv` let us pass additional parameters to our input `x`. They are randomly initialised. Note: this is not the keys from the encoder (if we are in a decoder setting) - they are passed as `keys` in the forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n",
    "num_mem_kv, mem_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allows for local attention in heads: https://github.com/lucidrains/local-attention. Not part of the reformer (even though the sorted attention chuncks uses a form of local attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.n_local_attn_heads = n_local_attn_heads\n",
    "# self.local_attn = LocalAttention(window_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n",
    "# self.callback = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1024, 128) # random data for testing\n",
    "keys = None\n",
    "input_mask = None       # padding\n",
    "input_attn_mask = None  # direct attention mask - remove\n",
    "context_mask = None     # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1024, 128, 8, 64, 0, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, dtype = x.device, x.dtype\n",
    "b, t, e, h, dh, m, l_h = *x.shape, heads, dim_head, num_mem_kv, n_local_attn_heads\n",
    "b, t, e, h, dh, m, l_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b - bs\n",
    "* t - sl\n",
    "* e - embedding dim\n",
    "* h - n_heads\n",
    "* dh - head dim\n",
    "* m - num_mem_k?\n",
    "* l_h - n_local_attn_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is mem_kv? Random parameters of [bs, mem_kv, sl] added to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 0, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_kv = default(mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "mem = mem_kv.expand(b, m, -1)\n",
    "mem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create empty keys if not passed in (depende on the role of the layer encoder/decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, torch.Size([10, 0, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "c = keys.shape[1]\n",
    "kv_len = t + m + c\n",
    "kv_len, keys.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to decide if can use full attention or not. Unsure of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use_full_attn = use_full_attn or kv_len <= full_attn_thres\n",
    "use_full_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cat `mem` and `keys` to x if they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.cat((x, mem, keys), dim=1) # why do we cat keys to input?\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection matrices to create shared `qk` and `values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 1024, 128]),\n",
       " torch.Size([10, 1024, 512]),\n",
       " torch.Size([10, 1024, 512]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk = toqk(x)\n",
    "v = tov(x)\n",
    "x.shape, qk.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat `v` across heads `one_value_head` is True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, torch.Size([10, 1024, 512]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = v.repeat(1, 1, v_head_repeats)   # repeat v if desired\n",
    "v_head_repeats, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split n_heads dim to position 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 8, 1024, 64]), torch.Size([10, 8, 1024, 64]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_batch_and_heads = partial(merge_dims, 0, 1)\n",
    "qk, v = map(merge_heads, (qk, v))\n",
    "qk.shape, v.shape   # [bs, n_heads, sl, head_dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide number of lsh-heads, depeding on number of local heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 8, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_local = l_h > 0\n",
    "lsh_h = h - l_h\n",
    "has_local, lsh_h, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 1024, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split local heads (if any) and normal heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 0, 1024, 64]),\n",
       " torch.Size([10, 8, 1024, 64]),\n",
       " torch.Size([10, 0, 1024, 64]),\n",
       " torch.Size([10, 8, 1024, 64]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_index_fn = partial(split_at_index, 1, l_h)\n",
    "(lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n",
    "lqk.shape, qk.shape, lv.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge batch and nheads dim. Note that `batch` and `n_heads` dimension are combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([0, 1024, 64]),\n",
       " torch.Size([80, 1024, 64]),\n",
       " torch.Size([0, 1024, 64]),\n",
       " torch.Size([80, 1024, 64]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n",
    "lqk.shape, qk.shape, lv.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with masks, assumed to be set up properly before passed in. This code concatenates the various masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {}\n",
    "if input_mask is not None or context_mask is not None:\n",
    "    default_mask = torch.tensor([True], device=device)\n",
    "    i_mask = default(input_mask, default_mask.expand(b, t))\n",
    "    m_mask = default_mask.expand(b, m)\n",
    "    c_mask = default(context_mask, default_mask.expand(b, c))\n",
    "    mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n",
    "    mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n",
    "    masks['input_mask'] = mask\n",
    "\n",
    "if input_attn_mask is not None:\n",
    "    input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n",
    "    masks['input_attn_mask'] = input_attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select attention function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSHAttention(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_fn = lsh_attn if not use_full_attn else full_attn\n",
    "attn_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use query len if set, in our case, t=sl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(LSHAttention(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "), query_len=1024)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_attn_fn = partial(attn_fn, query_len = t)\n",
    "partial_attn_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to process attention per head. The `attn_chunks` argument let's us adjust the chunk size. This will have memory/performance implications. Default is 1 `attn_chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.process_inputs_chunk.<locals>.inner_fn(*args, **kwargs)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = attn_chunks)\n",
    "attn_fn_in_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the attention function. **Note! our qk has batch and n_heads dimensions collapsed. We need to run LSH attention on one head at a time. `process_inputs_chunk` takes care of this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1024, 64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([80, 1024, 64]), torch.Size([0]), torch.Size([80, 8192]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n",
    "out.shape, attn.shape, buckets.shape # out: [bs*n_heads, sl, head_dim], buckets: [bs*n_heads, sl*n_rounds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe use callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if self.callback is not None:\n",
    "#     self.callback(attn.reshape(b, lsh_h, t, -1), buckets.reshape(b, lsh_h, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with local attention, we won't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_local:\n",
    "    lqk, lv = lqk[:, :t], lv[:, :t]\n",
    "    local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n",
    "    local_out = local_out.reshape(b, l_h, t, -1)\n",
    "    out = out.reshape(b, lsh_h, t, -1)\n",
    "    out = torch.cat((local_out, out), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally reshape output to [bs, sl, n_heads*head_dim]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = split_heads(out).view(b, t, -1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And process thru final linear layer. Note: if our n_heads*head_dim is greater than embedding_dim, this step will reshape the output to appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 128])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = to_out(out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return output, possibly with a final dropout - not standard to my knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_attn_dropout(out).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "365.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
