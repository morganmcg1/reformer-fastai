{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reformer_w_tests.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganmcg1/reformer-fastai/blob/main/exploration/reformer_w_tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8FAwZJOAwRK"
      },
      "source": [
        "# credits to @lucidrains https://github.com/lucidrains"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAZABbMKBBG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a47fd8-4684-496d-e5a6-9f315d2b873d"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install -qq einops axial-positional-embedding"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObF9-9S1AmY5"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from functools import partial, reduce\n",
        "from inspect import isfunction\n",
        "from operator import mul\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "import pdb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akuIe6gEEjsU"
      },
      "source": [
        "## Basic Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePUm0EFNEl2O"
      },
      "source": [
        "# credits to @lucidrains https://github.com/lucidrains\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from functools import partial, reduce\n",
        "from inspect import isfunction\n",
        "from operator import mul\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "try:\n",
        "    from axial_positional_embedding import AxialPositionalEmbedding, AxialPositionalEmbeddingImage\n",
        "except ImportError as e:\n",
        "    print(e)\n",
        "\n",
        "# helpers\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "def expand_dim1(x):\n",
        "    if len(x.shape) == 1:\n",
        "        return x[None, :]\n",
        "    else: return x\n",
        "\n",
        "# generative helpers\n",
        "# credit https://github.com/huggingface/transformers/blob/a0c62d249303a68f5336e3f9a96ecf9241d7abbe/src/transformers/generation_logits_process.py\n",
        "def top_p_filter(logits, top_p=0.9):\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "    sorted_indices_to_remove = cum_probs > top_p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    # if min_tokens_to_keep > 1:\n",
        "    #         # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "    #         sorted_indices_to_remove[..., : min_tokens_to_keep - 1] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "    scores[indices_to_remove] = float('-inf')\n",
        "    return scores\n",
        "\n",
        "def top_k_filter(logits, top_k=20):\n",
        "    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "    logits[indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "_sampler = {\n",
        "    'top_k':top_k_filter,\n",
        "    'top_p':top_p_filter,\n",
        "    'gready':lambda x: x.argmax(-1)\n",
        "}\n",
        "\n",
        "# axial position helpers (subjected to review)\n",
        "def get_axial_dims(dim, n):\n",
        "    res = (dim//n, )*(n-1)\n",
        "    res += (dim-sum(res), )\n",
        "    return res\n",
        "\n",
        "\"\"\"## Helpers and FeedForward\"\"\"\n",
        "\n",
        "# helper classes \n",
        "# based on https://github.com/lucidrains/all-normalization-transformer/blob/master/all_normalization_transformer/all_normalization_transformer.py\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "# Added *args, **kwargs here to pass context and masks\n",
        "class PostNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.fn(x, *args, **kwargs)\n",
        "        return self.norm(x)\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, *args, **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, d_ff=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        d_ff = default(d_ff, 4*dim)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\"\"\"## Attention\"\"\"\n",
        "\n",
        "MASK_VAL = -1e4 # instead of float('-inf') to make fp16 work\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, \n",
        "                 dim, \n",
        "                 heads = 8, \n",
        "                 causal = False,\n",
        "                 mask = None,\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.causal = causal\n",
        "        self.store_attention = False\n",
        "        self.mask = mask #??\n",
        "        self.heads = heads\n",
        "        self.scale = dim ** -0.5\n",
        "        \n",
        "        self.to_q = nn.Linear(dim, dim, bias = False)\n",
        "        self.to_kv = nn.Linear(dim, dim * 2, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, context = None, mask = None, context_mask = None, store_attention=False):\n",
        "        b, n, _, h, device = *x.shape, self.heads, x.device\n",
        "        kv_input = default(context, x)\n",
        "\n",
        "        q = self.to_q(x) # replaced q_ with q (don't need to store it fore basic tfmr)\n",
        "        kv = self.to_kv(kv_input).chunk(2, dim = -1)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, *kv))\n",
        "        # boolean input_mask is False at positions not to attend to\n",
        "        input_mask = None\n",
        "        if any(map(exists, (mask, context_mask))):\n",
        "            q_mask = default(mask, lambda: torch.ones((b, n), device = device).bool())\n",
        "            k_mask = q_mask if not exists(context) else context_mask\n",
        "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device = device).bool())\n",
        "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
        "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
        "            input_mask = q_mask * k_mask\n",
        "        # classic dot-product attention\n",
        "        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n",
        "        # might need to tune MASK_VAL for fp16 to work\n",
        "        if exists(input_mask):\n",
        "            dots.masked_fill_(~input_mask, MASK_VAL)\n",
        "            del input_mask\n",
        "\n",
        "        if self.causal:\n",
        "            i, j = dots.shape[-2:]\n",
        "            mask = torch.ones((i, j), device = device).triu_(j - i + 1).bool()\n",
        "            dots.masked_fill_(mask, MASK_VAL)\n",
        "            del mask\n",
        "\n",
        "        attn = F.softmax(dots, -1)\n",
        "        if self.store_attention: # and not self.training\n",
        "            self.attention = attn.detach().cpu()\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out =  self.to_out(out)\n",
        "        #out = self.dropout(out) # option for more dropout here\n",
        "        return out\n",
        "\n",
        "\n",
        "\"\"\"## Transformer blocks\n",
        "\n",
        "### Encoder\n",
        "\"\"\"\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bacis transformer encoder block. Consists of multi-head attention and positional feedforward layers\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads = 8, causal = False, mask = None, \n",
        "                 attn_dropout=0.1, ff_dropout=0.1, d_ff=None, prenorm=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = attn_dropout # mb separate argument\n",
        "        if prenorm:\n",
        "            self.attn = Residual(PreNorm(dim, Attention(dim, heads=heads, causal=causal, dropout=attn_dropout)))\n",
        "            self.ff = Residual(PreNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
        "        else:\n",
        "            self.attn = Residual(PostNorm(dim, Attention(dim, heads=heads, causal=causal, dropout=attn_dropout)))\n",
        "            self.ff = Residual(PostNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
        "    def forward(self, x, mask=None): #? more args\n",
        "        out = self.attn(x, mask=mask)\n",
        "        out = F.dropout(out, p=self.attn_dropout)\n",
        "        out = self.ff(out)\n",
        "        return out\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim, depth=6, heads=8, causal=False, d_ff=None, attn_dropout=0.1, ff_dropout=0.1, prenorm=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(TransformerEncoderBlock(dim, heads, causal=causal, d_ff=d_ff, \n",
        "                                    attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm))\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask=mask)\n",
        "        return x\n",
        "\n",
        "\"\"\"Decoder block has attention and cross attention\n",
        "\n",
        "### Decoder\n",
        "\"\"\"\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, mask = None, d_ff=None,\n",
        "                 attn_dropout=0.1, ff_dropout=0.1, prenorm=False):\n",
        "        super().__init__()\n",
        "        self.attn_dropout = attn_dropout # mb separate argument\n",
        "        if prenorm:\n",
        "            self.attn = Residual(PreNorm(dim, Attention(dim, heads=heads, causal=True, dropout=attn_dropout)))\n",
        "            self.cross = Residual(PreNorm(dim, Attention(dim, heads=heads, causal=False, dropout=attn_dropout)))\n",
        "            self.ff = Residual(PreNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
        "        else:\n",
        "            self.attn = Residual(PostNorm(dim, Attention(dim, heads=heads, causal=True, dropout=attn_dropout)))\n",
        "            self.cross = Residual(PostNorm(dim, Attention(dim, heads=heads, causal=False, dropout=attn_dropout)))\n",
        "            self.ff = Residual(PostNorm(dim, FeedForward(dim, d_ff=d_ff, dropout=ff_dropout)))\n",
        "    def forward(self, x, context, mask=None, context_mask=None):\n",
        "        out = self.attn(x, mask=mask)\n",
        "        out = F.dropout(out, p=self.attn_dropout)\n",
        "        out = self.cross(out, context, mask=mask, context_mask=context_mask)\n",
        "        out = F.dropout(out, p=self.attn_dropout)\n",
        "        out = self.ff(out)\n",
        "        return out\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, dim, depth=6, heads=8, d_ff=None, attn_dropout=0.1, ff_dropout=0.1, prenorm=False):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(TransformerDecoderBlock(dim, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=False))\n",
        "    def forward(self, x, context, mask=None, context_mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, context, mask, context_mask)\n",
        "        return x\n",
        "\n",
        "\"\"\"### Models\"\"\"\n",
        "# from https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py#L609\n",
        "\n",
        "class AbsolutePositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = torch.arange(x.shape[1], device=x.device)\n",
        "        return self.emb(t)\n",
        "\n",
        "class FixedPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = torch.arange(x.shape[1], device=x.device).type_as(self.inv_freq)\n",
        "        sinusoid_inp = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
        "        return emb[None, :, :]\n",
        "#TODO add axial positional encodings\n",
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Combines token embedings with positional encodings\n",
        "    pos_enc: str from {'absolute', 'fixed', 'axial'}\n",
        "    \"\"\"\n",
        "    def __init__(self, emb_sz, dim, max_seq_len=512, dropout=0., pos_enc='absolute', \n",
        "                 axial_shape=None, axial_emb_dims=None):\n",
        "        super().__init__()\n",
        "        self.scale = dim**0.5\n",
        "        self.emb = nn.Embedding(emb_sz, dim)\n",
        "        if pos_enc == 'absolute':\n",
        "            self.pos_enc = AbsolutePositionalEmbedding(dim, max_seq_len)\n",
        "        elif pos_enc == 'fixed':\n",
        "            self.pos_enc = FixedPositionalEmbedding(dim)\n",
        "        elif pos_enc == 'axial':\n",
        "            assert axial_shape is not None\n",
        "            assert reduce(mul, axial_shape) == max_seq_len\n",
        "            axial_emb_dims = default(axial_emb_dims, get_axial_dims(dim, len(axial_shape)))\n",
        "            self.pos_enc = AxialPositionalEmbedding(dim, axial_shape, axial_emb_dims)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self._init()\n",
        "    def forward(self, x):\n",
        "        _, n = x.shape\n",
        "        x = self.emb(x)\n",
        "        x *= self.scale\n",
        "        x += self.pos_enc(x)\n",
        "        return self.dropout(x)\n",
        "    def _init(self):\n",
        "        nn.init.normal_(self.emb.weight, std = 0.02)\n",
        "        if hasattr(self.pos_enc, 'weight'):\n",
        "            nn.init.normal_(self.pos_enc.weight, std = 0.02)\n",
        "\n",
        "#TODO test weight tying\n",
        "# Note on weight tying: it's done like here in fastai AWD_LSTM model\n",
        "# Lucidrains does it with custom MatrixMultiply module https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py#L106\n",
        "class TransformerEncDec(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic Transformer Encoder-Decoder model\n",
        "    Parameters:\n",
        "        * enc_vocab_sz: int - source vocab size \n",
        "        * dec_vocab_sz: int - target vocab size\n",
        "        * dim: int - inner dimension of the model\n",
        "        * depth: int (default: 6) \n",
        "        * heads: int (default: 8)\n",
        "        * max_seq_len: int (default: 512)\n",
        "        * pad_idx: int - padding token id, if pad_idx is provided, and no mask/context_mask are passed to \n",
        "                forward method will be used to generate padding masks\n",
        "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
        "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
        "    Inputs:\n",
        "        * src - source input ids, shape [bs, src_sl]\n",
        "        * tgt - target input ids, shape [bs, tgt_sl]\n",
        "        * src_mask - optional boolean source mask, shape [bs, src_sl]\n",
        "        * tgt_mask - optional boolean target mask, shape [bs, tgt_sl]\n",
        "    Returns:\n",
        "        * logits - target token logits, shape [bs, tgt_sl, tgt_vocab_sz]\n",
        "    \"\"\"\n",
        "    def __init__(self, enc_vocab_sz, dec_vocab_sz, dim, depth=6, heads=8, \n",
        "                 max_seq_len=512, pad_idx=None, tie_weights=True, \n",
        "                 attn_dropout=0.1, ff_dropout=0.1, emb_dropout=0.1,\n",
        "                 pos_enc='absolute', d_ff=None, prenorm=False, \n",
        "                 axial_shape=None, axial_emb_dims=None):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.depth = depth\n",
        "        self.pad_idx = pad_idx\n",
        "        self.enc_emb = TransformerEmbedding(enc_vocab_sz, dim, max_seq_len, dropout=emb_dropout,\n",
        "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
        "        self.dec_emb = TransformerEmbedding(dec_vocab_sz, dim, max_seq_len, dropout=emb_dropout,\n",
        "                                            axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
        "        self.encoder = TransformerEncoder(dim, depth, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm)\n",
        "        self.decoder = TransformerDecoder(dim, depth, heads, d_ff=d_ff, attn_dropout=attn_dropout, ff_dropout=ff_dropout, prenorm=prenorm)\n",
        "        self.proj = nn.Linear(dim, dec_vocab_sz)\n",
        "        if tie_weights: self.proj.weight = self.dec_emb.emb.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_mask = None, tgt_mask = None):\n",
        "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
        "        tgt_mask = default(tgt_mask, self.get_padding_mask(tgt))\n",
        "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
        "        out = self.decoder(self.dec_emb(tgt), context=enc, mask=tgt_mask, context_mask=src_mask)\n",
        "        return self.proj(out)\n",
        "    def get_padding_mask(self, x):\n",
        "        if self.pad_idx is None: return None\n",
        "        return (x != self.pad_idx)\n",
        "    #TODO add beam search and refactor\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src,\n",
        "                src_mask=None,\n",
        "                max_len=50,\n",
        "                temperature=1.,\n",
        "                method = 'top_k',\n",
        "                top_k = 20,\n",
        "                top_p = 0.9,\n",
        "                early_stopping=False,\n",
        "                bos_idx=2, # TODO change to match future usecases\n",
        "                eos_idx=None):\n",
        "        self.to(src.device) #TODO test for potential problems\n",
        "        self.eval()\n",
        "        thresh = top_k if method=='top_k' else top_p\n",
        "        sampler = _sampler[method]\n",
        "        src = expand_dim1(src)\n",
        "        bs = src.size(0)\n",
        "        inp = src.new_full((bs, 1), bos_idx) #start with bos tokens\n",
        "        pdb.set_trace()\n",
        "        src_mask = default(src_mask, self.get_padding_mask(src))\n",
        "        enc = self.encoder(self.enc_emb(src), mask = src_mask)\n",
        "        out = inp\n",
        "        for _ in range(max_len):\n",
        "            x = out[:, -self.max_seq_len:]\n",
        "            dec = self.decoder(self.dec_emb(out), context=enc)\n",
        "            logits = self.proj(dec)[:, -1, :]\n",
        "            if method == 'greedy':\n",
        "                sample = sampler(logits)\n",
        "            else:\n",
        "                filtered_logits = sampler(logits)\n",
        "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
        "                sample = torch.multinomial(probs, 1)\n",
        "\n",
        "            out = torch.cat((out, sample), dim=-1)\n",
        "\n",
        "            if (early_stopping and \n",
        "                ((sample == eos_idx).all() or \n",
        "                (sample == self.pad_idx).all())):\n",
        "                break\n",
        "        #TODO mb output cleanup\n",
        "        return out\n",
        "    \n",
        "    def store_attention(self, layer_ids=None, store_encoder=False, store_decoder=True):\n",
        "        #defaults to storing attention for all layers\n",
        "        layer_ids = default(layer_ids, list(range(self.depth)))\n",
        "        for module in self.children():\n",
        "            if issubclass(type(module), TransformerEncoder) and store_encoder:\n",
        "                for i, l in enumerate(module.layers):\n",
        "                    if i in layer_ids:\n",
        "                        for m in l.modules():\n",
        "                            if issubclass(type(m), (Attention)):\n",
        "                                m.store_attention = True\n",
        "            elif issubclass(type(module), TransformerDecoder) and store_decoder:\n",
        "                for i, l in enumerate(module.layers):\n",
        "                    if i in layer_ids:\n",
        "                        for m in l.modules():\n",
        "                            if issubclass(type(m), (Attention)):\n",
        "                                m.store_attention = True\n",
        "    #TODO mb separate encoder and decoder attention\n",
        "    def get_attention_matrix(self, get_encoder=False, get_decoder=True):\n",
        "        res = []\n",
        "        if get_encoder:\n",
        "            for m in self.encoder.modules():\n",
        "                if issubclass(type(m), (Attention)):\n",
        "                    attention = getattr(m, 'attention', None)\n",
        "                    if attention is not None:\n",
        "                        res.append(attention)\n",
        "                    # reset stored attention\n",
        "                    m.attention = None\n",
        "                    m.store_attention = False\n",
        "        if get_decoder:\n",
        "            for m in self.decoder.modules():\n",
        "                if issubclass(type(m), (Attention)):\n",
        "                    attention = getattr(m, 'attention', None)\n",
        "                    if attention is not None:\n",
        "                        res.append(attention)\n",
        "                    # reset stored attention\n",
        "                    m.attention = None\n",
        "                    m.store_attention = False\n",
        "        return res\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic Transformer for language modelling\n",
        "    Parameters:\n",
        "        * vocab_sz: int\n",
        "        * dim: int - inner dimension of the model\n",
        "        * depth: int (default: 6) \n",
        "        * heads: int (default: 8)\n",
        "        * causal: bool (default: True) - if True does causal masking automatically\n",
        "        * max_seq_len: int (default: 512)\n",
        "        * tie_weights: bool - if True target embedding weights are used for computation output projection\n",
        "        * pos_enc: str from {'absolute', 'fixed', 'axial'} - type of positional encoding to use\n",
        "    Inputs:\n",
        "        * x - input ids, shape [bs, sl]\n",
        "        * mask - optional boolean mask, shape [bs, sl]\n",
        "    Returns:\n",
        "        * logits - target token logits, shape [bs, sl, vocab_sz]\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_sz, dim, depth=6, heads=8, causal=True,\n",
        "                 max_seq_len=512, tie_weights=True, d_ff=None,\n",
        "                 attn_dropout=0.1, ff_dropout=0.1, emb_dropout=0.1,\n",
        "                 pos_enc='absolute', pad_idx=None, prenorm=False,\n",
        "                 axial_shape=None, axial_emb_dims=None):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.depth = depth\n",
        "        self.pad_idx = pad_idx\n",
        "        self.emb = TransformerEmbedding(vocab_sz, dim, max_seq_len, dropout=emb_dropout, \n",
        "                                        axial_shape=axial_shape, axial_emb_dims=axial_emb_dims)\n",
        "        self.tfmr = TransformerEncoder(dim, depth, heads, causal=causal, d_ff=d_ff, \n",
        "                                       attn_dropout=attn_dropout, ff_dropout=ff_dropout,\n",
        "                                       prenorm=prenorm)\n",
        "        self.proj = nn.Linear(dim, vocab_sz)\n",
        "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.emb(x)\n",
        "        x = self.tfmr(x, mask=mask)\n",
        "        return self.proj(x)\n",
        "    #TODO maybe refactor\n",
        "    @torch.no_grad()\n",
        "    def generate(self, inp,\n",
        "                max_len=50,\n",
        "                temperature=1.,\n",
        "                method = 'top_k',\n",
        "                top_k = 20,\n",
        "                top_p = 0.9,\n",
        "                early_stopping=False, #need eos_idx to work\n",
        "                eos_idx=None):\n",
        "        self.to(inp.device) #TODO test for potential problems\n",
        "        self.eval()\n",
        "        thresh = top_k if method=='top_k' else top_p\n",
        "        sampler = _sampler[method]\n",
        "        inp = expand_dim1(inp)\n",
        "        b, t = inp.shape\n",
        "        out = inp\n",
        "        for _ in range(max_len):\n",
        "            x = out[:, -self.max_seq_len:]\n",
        "\n",
        "            logits = self(x)[:, -1, :]\n",
        "            if method == 'greedy':\n",
        "                sample = sampler(logits)\n",
        "            else:\n",
        "                filtered_logits = sampler(logits)\n",
        "                probs = F.softmax(filtered_logits / temperature, dim=-1)\n",
        "                sample = torch.multinomial(probs, 1)\n",
        "\n",
        "            out = torch.cat((out, sample), dim=-1)\n",
        "\n",
        "            if early_stopping and (sample == eos_idx).all():\n",
        "                break\n",
        "        # out = out[:, t:]\n",
        "        return out\n",
        "    \n",
        "    def store_attention(self, layer_ids=None):\n",
        "        #defaults to storing attention for all layers\n",
        "        layer_ids = default(layer_ids, list(range(self.depth)))\n",
        "        for module in self.children():\n",
        "            if issubclass(type(module), (TransformerEncoder, TransformerDecoder)):\n",
        "                for i, l in enumerate(module.layers):\n",
        "                    if i in layer_ids:\n",
        "                        for m in l.modules():\n",
        "                            if issubclass(type(m), (Attention)):\n",
        "                                m.store_attention = True\n",
        "    def get_attention_matrix(self):\n",
        "        res = []\n",
        "        for m in self.modules():\n",
        "            if issubclass(type(m), (Attention)):\n",
        "                attention = getattr(m, 'attention', None)\n",
        "                if attention is not None:\n",
        "                    res.append(attention)\n",
        "                # reset stored attention\n",
        "                m.attention = None\n",
        "                m.store_attention = False\n",
        "        return res"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuoyB1_-ri_U"
      },
      "source": [
        "## Reformer healpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTsoqjI7YRBu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd.function import Function\n",
        "from torch.utils.checkpoint import get_device_states, set_device_states\n",
        "from functools import wraps"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzcIfYmq_Iom"
      },
      "source": [
        "def cache_fn(f):\n",
        "    cache = None\n",
        "    @wraps(f)\n",
        "    def cached_fn(*args, **kwargs):\n",
        "        nonlocal cache\n",
        "        if cache is not None:\n",
        "            return cache\n",
        "        cache = f(*args, **kwargs)\n",
        "        return cache\n",
        "    return cached_fn\n",
        "\n",
        "def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n",
        "    def inner_fn(fn):\n",
        "        @wraps(fn)\n",
        "        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n",
        "            namespace_str = str(default(key_namespace, ''))\n",
        "            _cache = getattr(self, cache_attr)\n",
        "            _keyname = f'{cache_namespace}:{namespace_str}'\n",
        "\n",
        "            if fetch:\n",
        "                val = _cache[_keyname]\n",
        "                if reexecute:\n",
        "                    fn(self, *args, **kwargs)\n",
        "            else:\n",
        "                val = fn(self, *args, **kwargs)\n",
        "                if set_cache:\n",
        "                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n",
        "            return val\n",
        "        return wrapper\n",
        "    return inner_fn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy4DysmI-r3C"
      },
      "source": [
        "class Chunk(nn.Module):\n",
        "    def __init__(self, chunks, fn, along_dim = -1):\n",
        "        super().__init__()\n",
        "        self.dim = along_dim\n",
        "        self.chunks = chunks\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        if self.chunks == 1:\n",
        "            return self.fn(x, **kwargs)\n",
        "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
        "        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQOiNyyKqKP_"
      },
      "source": [
        "class ChunkedFeedForward(nn.Module):\n",
        "    def __init__(self, d, ff_d=None, chunks=1, dropout=0., along_dim=-1):\n",
        "        super().__init__()\n",
        "        ff_d = default(ff_d, 4*d)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d, ff_d),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_d, d),\n",
        "            nn.Dropout(dropout)\n",
        "            )\n",
        "        self.chunks = chunks\n",
        "        self.dim = along_dim\n",
        "    def forward(self, x):\n",
        "        if self.chunks == 1:\n",
        "            return self.net(x)\n",
        "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
        "        return torch.cat([self.net(c) for c in chunks], dim = self.dim)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQHfbWXysNgk",
        "outputId": "ce1d4dc5-c6d8-4416-8c23-6a58d792b9c0"
      },
      "source": [
        "bs = 8\n",
        "sl = 512\n",
        "d = 64\n",
        "x = torch.randn(bs, sl, d)\n",
        "ff  = ChunkedFeedForward(d, chunks=8, along_dim=1)\n",
        "out = ff(x)\n",
        "out.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkR45Gk8s7UI",
        "outputId": "28a10521-8016-4ea5-bc5f-014703c5d674"
      },
      "source": [
        "bs = 8\n",
        "sl = 512\n",
        "d = 64\n",
        "x = torch.randn(bs, sl, d)\n",
        "ff  = PostNorm(d, ChunkedFeedForward(d, chunks=8, along_dim=1))\n",
        "out = ff(x)\n",
        "out.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 512, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVytFZjbYM1H"
      },
      "source": [
        "## Reversible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqDaOGzYZtwG"
      },
      "source": [
        "# following example for saving and setting rng here https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\n",
        "class Deterministic(nn.Module):\n",
        "    def __init__(self, net):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.cpu_state = None\n",
        "        self.cuda_in_fwd = None\n",
        "        self.gpu_devices = None\n",
        "        self.gpu_states = None\n",
        "\n",
        "    def record_rng(self, *args):\n",
        "        self.cpu_state = torch.get_rng_state()\n",
        "        if torch.cuda._initialized:\n",
        "            self.cuda_in_fwd = True\n",
        "            self.gpu_devices, self.gpu_states = get_device_states(*args)\n",
        "\n",
        "    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n",
        "        if record_rng:\n",
        "            self.record_rng(*args)\n",
        "\n",
        "        if not set_rng:\n",
        "            return self.net(*args, **kwargs)\n",
        "\n",
        "        rng_devices = []\n",
        "        if self.cuda_in_fwd:\n",
        "            rng_devices = self.gpu_devices\n",
        "\n",
        "        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n",
        "            torch.set_rng_state(self.cpu_state)\n",
        "            if self.cuda_in_fwd:\n",
        "                set_device_states(self.gpu_devices, self.gpu_states)\n",
        "            return self.net(*args, **kwargs)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMEvcH51ZvrN"
      },
      "source": [
        "# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n",
        "# once multi-GPU is confirmed working, refactor and send PR back to source\n",
        "class ReversibleBlock(nn.Module):\n",
        "    def __init__(self, f, g, depth=None, send_signal = False):\n",
        "        super().__init__()\n",
        "        self.f = Deterministic(f)\n",
        "        self.g = Deterministic(g)\n",
        "\n",
        "        self.depth = depth\n",
        "        self.send_signal = send_signal\n",
        "\n",
        "    def forward(self, x, f_args = {}, g_args = {}):\n",
        "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
        "        y1, y2 = None, None\n",
        "\n",
        "        if self.send_signal:\n",
        "            f_args['_reverse'] = g_args['_reverse'] = False\n",
        "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n",
        "            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n",
        "\n",
        "        return torch.cat([y1, y2], dim=2)\n",
        "\n",
        "    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n",
        "        y1, y2 = torch.chunk(y, 2, dim=2)\n",
        "        del y\n",
        "\n",
        "        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n",
        "        del dy\n",
        "\n",
        "        if self.send_signal:\n",
        "            f_args['_reverse'] = g_args['_reverse'] = True\n",
        "            f_args['_depth'] = g_args['_depth'] = self.depth\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            y1.requires_grad = True\n",
        "            gy1 = self.g(y1, set_rng=True, **g_args)\n",
        "            torch.autograd.backward(gy1, dy2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x2 = y2 - gy1\n",
        "            del y2, gy1\n",
        "\n",
        "            dx1 = dy1 + y1.grad\n",
        "            del dy1\n",
        "            y1.grad = None\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            x2.requires_grad = True\n",
        "            fx2 = self.f(x2, set_rng=True, **f_args)\n",
        "            torch.autograd.backward(fx2, dx1, retain_graph=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x1 = y1 - fx2\n",
        "            del y1, fx2\n",
        "\n",
        "            dx2 = dy2 + x2.grad\n",
        "            del dy2\n",
        "            x2.grad = None\n",
        "\n",
        "            x = torch.cat([x1, x2.detach()], dim=2)\n",
        "            dx = torch.cat([dx1, dx2], dim=2)\n",
        "\n",
        "        return x, dx\n",
        "\n",
        "class IrreversibleBlock(nn.Module):\n",
        "    def __init__(self, f, g):\n",
        "        super().__init__()\n",
        "        self.f = f\n",
        "        self.g = g\n",
        "\n",
        "    def forward(self, x, f_args={}, g_args={}):\n",
        "        x1, x2 = torch.chunk(x, 2, dim=2)\n",
        "        y1 = x1 + self.f(x2, **f_args)\n",
        "        y2 = x2 + self.g(y1, **g_args)\n",
        "        return torch.cat([y1, y2], dim=2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7fgoOFekHDN"
      },
      "source": [
        "bs = 8\n",
        "sl = 1024\n",
        "d_model = 64\n",
        "x = torch.randn(bs, sl, d_model)\n",
        "# revblock is called on twin x\n",
        "x2 = torch.cat([x, x], dim=-1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnhLb1JBkHDN"
      },
      "source": [
        "# det_ff = Deterministic(FeedForward(d_model))\n",
        "# out = det_ff(x)\n",
        "# out.shape"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZnxHbnUkHDN",
        "outputId": "e3585fc7-7686-4f53-c91b-db7faf5ef86c"
      },
      "source": [
        "attn = Attention(d_model)\n",
        "ff = FeedForward(d_model)\n",
        "revblock = ReversibleBlock(attn, ff)\n",
        "out = revblock(x2) #kills kernel...\n",
        "out.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-bpj3p2HxrJ",
        "outputId": "31173379-4e9c-4266-91b0-47ef6bde23c4"
      },
      "source": [
        "# no grads are stored\n",
        "out = torch.stack(out.chunk(2, dim=-1)).mean(dim=0)\n",
        "try:\n",
        "    out.mean().backward()\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOD4hnfOkHDN",
        "outputId": "3fb55a11-b28a-49d6-d98a-e3abbc943384"
      },
      "source": [
        "bs = 8\n",
        "sl = 1024\n",
        "d_model = 64\n",
        "x = torch.randn(bs, sl, d_model)\n",
        "x2 = torch.cat([x, x], dim=-1)\n",
        "attn = Attention(d_model)\n",
        "ff = FeedForward(d_model)\n",
        "irrevblock = IrreversibleBlock(attn, ff)\n",
        "out = irrevblock(x2)\n",
        "out.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WynJhbG6IuKX"
      },
      "source": [
        "out = torch.stack(out.chunk(2, dim=-1)).mean(dim=0)\n",
        "out.mean().backward()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGZB_tiUZx-z"
      },
      "source": [
        "class _ReversibleFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, blocks, kwargs):\n",
        "        ctx.kwargs = kwargs\n",
        "        for block in blocks:\n",
        "            x = block(x, **kwargs)\n",
        "        ctx.y = x.detach()\n",
        "        ctx.blocks = blocks\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dy):\n",
        "        y = ctx.y\n",
        "        kwargs = ctx.kwargs\n",
        "        for block in ctx.blocks[::-1]:\n",
        "            y, dy = block.backward_pass(y, dy, **kwargs)\n",
        "        return dy, None, None"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA-37csGJFEq",
        "outputId": "5d1f2f10-6cab-4859-b63c-310bb6a9f817"
      },
      "source": [
        "blocks = nn.ModuleList([revblock])\n",
        "out = _ReversibleFunction.apply(x2, blocks, {})\n",
        "out.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9Gv6xoyhug"
      },
      "source": [
        "### Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH1t8-FEMozf"
      },
      "source": [
        "class SimpleRevModel(nn.Module):\n",
        "    def __init__(self, d_model, d_emb):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(d_emb, d_model)\n",
        "        blocks = []\n",
        "        for i in range(2):\n",
        "            attn = Attention(d_model)\n",
        "            ff = FeedForward(d_model)\n",
        "            f = PreNorm(d_model, attn)\n",
        "            g = PreNorm(d_model, ff)\n",
        "            blocks.append(ReversibleBlock(f, g))\n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.proj = nn.Linear(d_model, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x = torch.cat([x, x], dim = -1)\n",
        "        x = _ReversibleFunction.apply(x, self.blocks, {})\n",
        "        x = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
        "        return self.proj(x)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRdwpu_7N2sN"
      },
      "source": [
        "bs = 8\n",
        "sl = 1024\n",
        "d_model = 64\n",
        "d_emb = 256\n",
        "x = torch.randint(d_emb-1, (bs, sl))\n",
        "m = SimpleRevModel(d_model, d_emb)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD4cKytYPRX4"
      },
      "source": [
        "out = m(x)\n",
        "loss = out.mean()\n",
        "loss.backward()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4S4bjuzQFfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f356bbb-12cd-4430-8414-af7b31e01602"
      },
      "source": [
        "for p in m.parameters():\n",
        "    print('-----------------')\n",
        "    print('param:')\n",
        "    print(p)\n",
        "    print('grad:')\n",
        "    print(p.grad)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 1.5495,  1.5794,  0.1363,  ...,  0.5427, -1.0125, -0.7630],\n",
            "        [ 0.6006, -0.9933, -1.7427,  ...,  0.0876,  0.8965, -1.5685],\n",
            "        [ 0.3045,  0.1133, -0.2850,  ..., -2.3012,  0.2390, -1.0908],\n",
            "        ...,\n",
            "        [ 1.5190, -0.2518,  1.2081,  ..., -0.0911,  1.5247,  1.8852],\n",
            "        [-0.0813, -1.2661, -1.7499,  ...,  1.4240, -1.7977,  0.3241],\n",
            "        [-0.5610, -0.1188,  0.7415,  ...,  0.8673, -0.4423, -0.1855]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-2.5323e-04, -4.8537e-04, -1.6292e-05,  ...,  1.9009e-04,\n",
            "          4.3628e-05, -3.1354e-04],\n",
            "        [-2.2829e-04, -4.9403e-04, -6.3573e-05,  ...,  2.5382e-04,\n",
            "          1.1339e-05, -3.8109e-04],\n",
            "        [-1.4337e-04, -4.1476e-04, -4.4840e-05,  ...,  1.7325e-04,\n",
            "         -2.4131e-05, -2.7153e-04],\n",
            "        ...,\n",
            "        [-2.8136e-04, -5.5855e-04, -5.7023e-05,  ...,  2.7608e-04,\n",
            "          3.6561e-05, -3.9074e-04],\n",
            "        [-1.6048e-04, -4.8522e-04, -6.9722e-05,  ...,  2.6264e-04,\n",
            "         -2.5830e-05, -3.4197e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0151,  0.0956,  0.1064,  ...,  0.0269,  0.1144,  0.0490],\n",
            "        [-0.0744,  0.0063, -0.0212,  ..., -0.0577, -0.1192, -0.0685],\n",
            "        [-0.0725, -0.0649, -0.0514,  ..., -0.0304, -0.0829, -0.1210],\n",
            "        ...,\n",
            "        [-0.0607,  0.0837,  0.0615,  ...,  0.1051, -0.1082,  0.0277],\n",
            "        [-0.0338,  0.0881, -0.0764,  ..., -0.0461,  0.0953, -0.0022],\n",
            "        [ 0.0304, -0.0356,  0.1016,  ..., -0.0948, -0.0358,  0.0066]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 2.6921e-05, -2.8909e-07,  1.0778e-05,  ...,  9.3349e-06,\n",
            "          1.2307e-05, -3.0372e-05],\n",
            "        [ 3.2224e-05, -5.3263e-07,  3.1014e-05,  ...,  1.2889e-05,\n",
            "         -7.4405e-06, -6.4183e-06],\n",
            "        [ 2.8379e-06,  1.1247e-05,  2.7158e-05,  ...,  6.8870e-06,\n",
            "         -6.6764e-06,  1.1911e-05],\n",
            "        ...,\n",
            "        [-1.9594e-05,  4.8353e-07,  2.4047e-06,  ..., -7.4163e-06,\n",
            "          2.1574e-06,  5.6827e-06],\n",
            "        [ 2.7712e-05,  9.2785e-06,  4.0340e-06,  ...,  2.3377e-06,\n",
            "          4.1568e-06,  4.5563e-06],\n",
            "        [ 2.4386e-06,  4.8251e-06,  1.1960e-05,  ...,  5.4789e-06,\n",
            "         -2.1868e-06, -1.1721e-06]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[-0.0752, -0.0181,  0.0249,  ...,  0.0119, -0.0635, -0.1161],\n",
            "        [-0.0698,  0.0490,  0.0687,  ..., -0.1234,  0.0230,  0.0371],\n",
            "        [-0.0679,  0.0588,  0.0844,  ..., -0.0545,  0.0445,  0.0931],\n",
            "        ...,\n",
            "        [ 0.0553, -0.0248, -0.0922,  ...,  0.0097, -0.0442,  0.0521],\n",
            "        [-0.0423, -0.1011, -0.0847,  ...,  0.1037,  0.1189, -0.1080],\n",
            "        [-0.0455, -0.0985, -0.0577,  ...,  0.0942,  0.0384,  0.0158]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-7.8846e-06,  3.1838e-05, -3.7283e-05,  ..., -1.1019e-05,\n",
            "          2.1318e-05, -1.0945e-05],\n",
            "        [-1.7339e-05,  9.6253e-06,  3.0981e-05,  ..., -8.4650e-06,\n",
            "          1.5098e-05,  1.6835e-05],\n",
            "        [ 4.7836e-06, -2.9564e-05,  4.5110e-05,  ...,  2.1967e-05,\n",
            "          7.5045e-06,  4.2911e-05],\n",
            "        ...,\n",
            "        [ 9.8881e-04,  1.7369e-04,  6.9905e-04,  ...,  3.6915e-04,\n",
            "         -3.6626e-04,  7.3424e-05],\n",
            "        [ 1.1364e-03,  1.9808e-04,  8.0300e-04,  ...,  4.0994e-04,\n",
            "         -4.2791e-04,  6.2893e-05],\n",
            "        [ 2.9379e-03,  5.3302e-04,  2.0760e-03,  ...,  1.0841e-03,\n",
            "         -1.0975e-03,  1.7384e-04]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0997, -0.0135,  0.0784,  ...,  0.0078, -0.1012, -0.1198],\n",
            "        [ 0.0574, -0.0872, -0.0724,  ..., -0.0579, -0.0435, -0.0802],\n",
            "        [ 0.0632, -0.0109,  0.1234,  ..., -0.0004,  0.0234,  0.0581],\n",
            "        ...,\n",
            "        [-0.0603, -0.0445, -0.0364,  ..., -0.0224, -0.0043, -0.0306],\n",
            "        [ 0.0733, -0.0761,  0.0484,  ..., -0.1111,  0.0576,  0.0084],\n",
            "        [ 0.0776,  0.0678,  0.0533,  ..., -0.0248, -0.0597, -0.0863]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-0.0014, -0.0008,  0.0008,  ..., -0.0005, -0.0005, -0.0019],\n",
            "        [-0.0040, -0.0022,  0.0024,  ..., -0.0014, -0.0015, -0.0053],\n",
            "        [ 0.0007,  0.0004, -0.0004,  ...,  0.0003,  0.0003,  0.0010],\n",
            "        ...,\n",
            "        [ 0.0022,  0.0012, -0.0013,  ...,  0.0008,  0.0008,  0.0029],\n",
            "        [ 0.0004,  0.0002, -0.0002,  ...,  0.0001,  0.0001,  0.0005],\n",
            "        [-0.0025, -0.0013,  0.0015,  ..., -0.0009, -0.0009, -0.0033]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([-0.1031,  0.0946, -0.0655, -0.1017,  0.0304, -0.0528,  0.0276,  0.1229,\n",
            "         0.0557,  0.0191, -0.0447, -0.0012,  0.0174,  0.0406,  0.0131,  0.0515,\n",
            "        -0.0430,  0.1189, -0.0125,  0.0516,  0.0376, -0.0798, -0.0636, -0.0132,\n",
            "        -0.0210,  0.0307,  0.0621, -0.0285,  0.0618, -0.0442,  0.1182,  0.0800,\n",
            "         0.0562,  0.0649,  0.1152,  0.0802,  0.0661,  0.1122, -0.0926, -0.0694,\n",
            "        -0.0836, -0.1216,  0.0878,  0.1249, -0.0985,  0.0338, -0.1019,  0.1109,\n",
            "         0.0058, -0.0099,  0.1172, -0.0691,  0.0721,  0.0036,  0.0698, -0.1186,\n",
            "         0.0024,  0.0703,  0.0744,  0.1164, -0.0122, -0.0565,  0.1007, -0.0008],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-0.0238, -0.0680,  0.0124, -0.0169,  0.0500,  0.0200, -0.0638,  0.0364,\n",
            "         0.0254,  0.0182, -0.0317,  0.0477,  0.0462,  0.0660, -0.0083, -0.0364,\n",
            "         0.0261, -0.0382, -0.0338, -0.0055,  0.0318,  0.0268,  0.0019, -0.0125,\n",
            "        -0.0376,  0.0268,  0.0057, -0.0089, -0.0216, -0.0104,  0.0253,  0.0497,\n",
            "        -0.0040,  0.0369, -0.0675,  0.0545,  0.0397, -0.0177, -0.0309, -0.0526,\n",
            "         0.0151, -0.0643,  0.0226,  0.0563, -0.0074,  0.0319, -0.0065, -0.0219,\n",
            "         0.0358,  0.0313, -0.0604,  0.0237,  0.0559,  0.0110, -0.0502,  0.0326,\n",
            "        -0.0107, -0.0613,  0.0336,  0.0248,  0.0179,  0.0369,  0.0067, -0.0417])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
            "grad:\n",
            "tensor([ 3.6932e-04, -3.2158e-06,  1.0317e-04, -3.0121e-04, -1.2786e-03,\n",
            "         3.1544e-05,  1.3046e-03, -1.3003e-03,  3.9872e-04, -4.3790e-05,\n",
            "         4.8614e-04,  6.8843e-05, -2.8131e-04,  3.7758e-04,  5.2904e-04,\n",
            "        -1.5131e-04,  5.6415e-04,  1.3112e-03, -5.6525e-04,  6.6367e-05,\n",
            "        -4.0567e-03, -2.7597e-04, -2.0898e-04, -2.2188e-04, -2.0728e-04,\n",
            "         1.8127e-05, -2.2804e-04,  1.5244e-05,  3.5870e-05, -2.2414e-04,\n",
            "        -7.2581e-04,  8.4073e-04,  1.0095e-03, -7.1481e-04,  2.2506e-04,\n",
            "        -2.6747e-04, -6.9903e-05,  3.0741e-04, -7.1703e-04,  2.3078e-03,\n",
            "         3.6907e-05,  1.7212e-03,  1.6373e-03,  1.6423e-03, -1.0362e-03,\n",
            "         9.8502e-04,  2.7776e-04,  5.3279e-04, -2.8468e-04,  1.9378e-05,\n",
            "         1.5660e-04, -3.3881e-04, -1.3155e-04,  5.0311e-04,  1.2992e-03,\n",
            "         3.7584e-05,  2.5348e-04, -5.0772e-04, -2.1073e-04, -6.4135e-05,\n",
            "         3.8608e-05,  1.3012e-06, -1.8471e-04, -1.5629e-05])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([ 0.0067, -0.0027,  0.0030, -0.0132,  0.0145,  0.0020, -0.0142, -0.0148,\n",
            "        -0.0122,  0.0007, -0.0170,  0.0070, -0.0112,  0.0077,  0.0180,  0.0037,\n",
            "        -0.0067,  0.0146, -0.0050, -0.0017,  0.0306,  0.0024,  0.0139,  0.0042,\n",
            "        -0.0043, -0.0018, -0.0056,  0.0003,  0.0021,  0.0156, -0.0092, -0.0208,\n",
            "        -0.0095, -0.0097, -0.0024, -0.0031, -0.0046, -0.0032, -0.0074, -0.0217,\n",
            "        -0.0005,  0.0113, -0.0196, -0.0117,  0.0167,  0.0167,  0.0218,  0.0047,\n",
            "         0.0030, -0.0003, -0.0217, -0.0084,  0.0062,  0.0110, -0.0159,  0.0139,\n",
            "         0.0026,  0.0164, -0.0021,  0.0013,  0.0044,  0.0012,  0.0092,  0.0009])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0165, -0.0093,  0.0408,  ..., -0.0619, -0.0995,  0.0560],\n",
            "        [ 0.0849,  0.1004, -0.0627,  ..., -0.0584,  0.0956, -0.0965],\n",
            "        [ 0.0658,  0.0900, -0.0128,  ..., -0.0907, -0.0645,  0.0991],\n",
            "        ...,\n",
            "        [ 0.0397, -0.1171,  0.1167,  ...,  0.0385,  0.0688, -0.1079],\n",
            "        [-0.0632,  0.1035, -0.1224,  ...,  0.0004, -0.0086, -0.0078],\n",
            "        [ 0.0452,  0.0236,  0.0409,  ...,  0.0014, -0.0742, -0.0772]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 2.6794e-04, -1.1813e-04, -2.8134e-04,  ...,  2.4224e-04,\n",
            "          3.8826e-04, -1.8077e-04],\n",
            "        [-6.9027e-04, -2.1190e-03,  1.5767e-03,  ...,  1.4523e-03,\n",
            "         -2.7024e-03,  1.5495e-03],\n",
            "        [-3.1057e-04, -7.7504e-04,  2.6683e-04,  ...,  5.9215e-04,\n",
            "         -4.0455e-05, -3.4447e-04],\n",
            "        ...,\n",
            "        [ 3.0133e-04, -4.7066e-04,  6.1761e-04,  ...,  3.2720e-04,\n",
            "          1.2224e-03, -1.0498e-03],\n",
            "        [ 1.0919e-03, -2.0135e-03,  2.1800e-03,  ...,  1.0930e-03,\n",
            "         -1.2408e-03,  3.3951e-04],\n",
            "        [-2.3400e-05,  3.5953e-05,  5.8828e-05,  ...,  1.3243e-05,\n",
            "          1.2102e-05, -5.7138e-05]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([ 0.1093,  0.0629, -0.0035, -0.1237,  0.0501, -0.0930,  0.0942, -0.0466,\n",
            "        -0.0752,  0.0150,  0.0263, -0.0457, -0.0674, -0.0982,  0.0050, -0.1118,\n",
            "        -0.0010,  0.0005,  0.1120, -0.0083, -0.0319, -0.1178,  0.0832,  0.0119,\n",
            "         0.1018,  0.0235, -0.1213, -0.1176,  0.0124,  0.0123,  0.0030, -0.1184,\n",
            "         0.0055, -0.0454, -0.0218, -0.1009,  0.0348,  0.0096, -0.1140,  0.0627,\n",
            "         0.1170, -0.0448,  0.0022,  0.0677, -0.0335, -0.1059,  0.0560,  0.0333,\n",
            "        -0.0569, -0.0708,  0.1123, -0.1177,  0.0825,  0.0151, -0.0172, -0.0772,\n",
            "        -0.0851, -0.0331,  0.1167, -0.0441, -0.0329,  0.0986, -0.0007, -0.1021,\n",
            "         0.0731,  0.0270, -0.0947,  0.0641,  0.1102,  0.0431, -0.0752,  0.0227,\n",
            "         0.0959, -0.0801,  0.0153, -0.0102, -0.0165, -0.1173,  0.0109, -0.0787,\n",
            "         0.0236, -0.0972, -0.0721, -0.0723,  0.0655,  0.0883, -0.0158, -0.0544,\n",
            "         0.0282, -0.1092,  0.0668,  0.0394,  0.0964, -0.0529,  0.0964, -0.0714,\n",
            "         0.0345,  0.0644, -0.1182, -0.0113, -0.0117,  0.0137, -0.1093,  0.0697,\n",
            "        -0.0395,  0.0137, -0.0454,  0.0043,  0.0973, -0.0349,  0.0399,  0.0385,\n",
            "         0.0709,  0.0980, -0.0391, -0.0058, -0.0408, -0.0290,  0.1195, -0.0815,\n",
            "        -0.0546,  0.0672, -0.0561, -0.1027,  0.1237, -0.1034,  0.1040, -0.0460,\n",
            "         0.0016, -0.0940, -0.0622, -0.0817, -0.0366,  0.0508, -0.1017,  0.1004,\n",
            "        -0.1226, -0.0884,  0.0989,  0.0325, -0.0064, -0.0115,  0.1142,  0.0891,\n",
            "         0.1097, -0.0522, -0.0448, -0.0762,  0.0920,  0.0330,  0.0393, -0.0866,\n",
            "        -0.0213, -0.0663, -0.0109, -0.0636, -0.0880,  0.0715,  0.0513, -0.0776,\n",
            "        -0.0176, -0.0004, -0.0984,  0.0473, -0.0984,  0.0020, -0.1179, -0.0984,\n",
            "        -0.0334, -0.0818, -0.1042, -0.0538,  0.0153, -0.1118,  0.0166, -0.0709,\n",
            "         0.0322,  0.0158, -0.1091,  0.0588,  0.0892,  0.0499, -0.0096,  0.0584,\n",
            "        -0.0292,  0.0092,  0.1202, -0.0849,  0.1074, -0.1180, -0.1234, -0.0574,\n",
            "         0.0562, -0.0866,  0.0610,  0.0618, -0.0328,  0.0480, -0.0809,  0.0824,\n",
            "         0.1044,  0.0673,  0.0304,  0.0325,  0.1130,  0.0682, -0.1172, -0.0371,\n",
            "        -0.0945, -0.0914, -0.0435,  0.0805, -0.0202,  0.1030, -0.0732,  0.0652,\n",
            "        -0.1111, -0.0558,  0.0676,  0.1126, -0.0950, -0.1244, -0.0020,  0.0641,\n",
            "        -0.0866,  0.0969,  0.0725, -0.0300,  0.0543,  0.0398, -0.0365,  0.0154,\n",
            "        -0.0998,  0.0695,  0.0702,  0.0368,  0.0656,  0.0934, -0.0033,  0.0674,\n",
            "         0.1241,  0.0532,  0.0795,  0.1105, -0.0996, -0.0293,  0.0160, -0.0923,\n",
            "        -0.0824, -0.0261,  0.0741, -0.0185,  0.0465,  0.0950, -0.0603, -0.1038],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-3.1107e-03, -1.0579e-02, -5.1500e-03, -1.2149e-03,  9.4486e-03,\n",
            "        -7.3401e-03,  1.1283e-02, -8.2723e-03,  5.2175e-03, -1.9684e-03,\n",
            "         1.1098e-02, -5.3076e-03, -5.3191e-04, -9.5925e-04,  6.5108e-03,\n",
            "        -1.4612e-03,  3.6654e-03,  7.3604e-03,  1.5077e-03,  7.3361e-03,\n",
            "        -7.7909e-04,  1.0659e-03, -7.2021e-03,  7.3327e-03, -5.4870e-03,\n",
            "         3.6436e-03, -4.4503e-03, -2.3970e-03,  2.1768e-03, -2.9812e-03,\n",
            "        -3.0370e-03, -2.7977e-03, -7.1019e-03,  2.4671e-03, -8.1707e-03,\n",
            "        -5.0300e-03, -4.1229e-03, -7.3486e-03,  8.5700e-03,  7.3168e-03,\n",
            "        -7.7252e-04, -3.2597e-03,  1.0252e-02,  4.0721e-03, -1.0628e-04,\n",
            "        -3.6212e-03,  5.9909e-03,  8.4182e-04, -4.4401e-03, -3.2688e-03,\n",
            "         6.0254e-03, -3.6198e-04,  1.3715e-02, -3.6200e-03,  2.5501e-03,\n",
            "        -6.4077e-03, -4.2355e-03, -6.1615e-03,  3.5977e-03, -2.5461e-03,\n",
            "        -1.3342e-04,  7.2881e-03, -6.1837e-03, -8.0950e-04, -5.5189e-03,\n",
            "        -1.5864e-02,  1.5612e-03,  7.9332e-03, -4.0758e-03,  6.8163e-04,\n",
            "        -7.3347e-03,  6.5881e-03,  3.7155e-05, -2.0117e-03,  1.8338e-03,\n",
            "        -5.0852e-03, -5.5703e-03,  1.5391e-03,  2.2053e-03,  1.6292e-03,\n",
            "        -2.7699e-03, -6.2891e-03,  8.4825e-03, -4.7229e-04,  3.1234e-04,\n",
            "         6.9605e-03,  4.9191e-03, -1.2244e-03, -1.1838e-02, -1.6779e-03,\n",
            "         4.8868e-03,  1.1668e-03,  3.4112e-04, -6.4122e-04,  2.9609e-03,\n",
            "        -6.7489e-03, -4.4142e-03,  3.1134e-03, -4.8894e-03,  3.7179e-03,\n",
            "        -9.4134e-04,  8.9280e-04,  4.5388e-03,  4.7285e-03,  1.2781e-03,\n",
            "        -7.3344e-03, -3.2811e-03, -3.8634e-03, -4.6348e-03,  4.9599e-03,\n",
            "        -6.8698e-03,  3.4940e-03, -7.0646e-03, -9.2930e-04, -4.5838e-03,\n",
            "         1.2593e-03, -3.7814e-03, -6.2984e-03, -4.3221e-04,  2.2910e-03,\n",
            "         7.4013e-03,  5.0339e-03, -9.7477e-04,  1.1168e-03,  6.4294e-03,\n",
            "        -6.3291e-03, -5.5534e-03, -2.7617e-03,  5.2490e-04,  2.8241e-03,\n",
            "         6.7788e-03,  6.1496e-03, -4.6341e-03, -4.5680e-06,  6.8612e-03,\n",
            "         4.7577e-03, -3.9636e-03, -2.3823e-03, -1.2787e-03,  3.9345e-03,\n",
            "         7.7860e-03, -3.0545e-03, -5.8320e-03, -4.5893e-04,  8.1541e-04,\n",
            "         3.8443e-03,  4.3795e-03,  4.3031e-03,  3.6672e-03,  4.6807e-04,\n",
            "        -4.4473e-03,  2.2728e-03,  1.3504e-03,  8.6124e-03, -2.2839e-03,\n",
            "        -4.2132e-03, -5.0081e-03, -2.8439e-03,  6.1567e-03, -7.7898e-03,\n",
            "         2.7422e-04, -5.7764e-04, -1.5137e-03,  6.2359e-03,  4.6083e-03,\n",
            "         9.3614e-03, -4.6257e-04,  6.9880e-03, -9.6804e-03, -2.3503e-04,\n",
            "        -8.4197e-04,  1.9493e-03,  6.1175e-03, -3.5627e-03,  7.3634e-03,\n",
            "        -8.8084e-03,  1.0686e-02, -3.8998e-03, -2.7860e-03, -1.9463e-03,\n",
            "         3.7509e-03,  1.0929e-04,  8.7986e-03,  9.9649e-03, -1.0651e-02,\n",
            "        -3.5419e-03, -3.9081e-04, -1.3327e-03, -1.1083e-02,  3.3658e-03,\n",
            "        -5.5938e-03,  8.9141e-03,  5.0004e-03,  1.7631e-03, -3.2506e-03,\n",
            "        -5.7129e-03, -5.1583e-03,  8.2215e-03,  3.2458e-03,  6.3468e-04,\n",
            "         4.0925e-03,  8.1639e-03, -4.9166e-03,  5.2479e-03, -4.4344e-03,\n",
            "         2.9167e-03, -4.7399e-03, -3.4134e-03, -9.5920e-04, -8.9603e-04,\n",
            "        -4.4521e-03, -2.0218e-03, -2.0643e-04, -6.7194e-04, -1.7965e-03,\n",
            "        -1.2519e-02,  2.8024e-03, -6.7124e-03,  3.0920e-03, -6.3474e-03,\n",
            "         4.1728e-03, -3.1719e-03, -5.9282e-03, -1.4916e-03, -5.1227e-03,\n",
            "        -3.6096e-03,  2.8547e-04, -3.3689e-04, -9.2952e-03, -3.9515e-03,\n",
            "        -1.7167e-03,  4.7860e-03, -4.8211e-04,  3.0127e-03,  7.0644e-04,\n",
            "         2.0320e-03, -2.8371e-03,  2.7382e-03, -3.9385e-03,  1.9472e-04,\n",
            "        -4.7995e-03,  4.4898e-03, -9.9755e-03, -4.5002e-03, -1.9184e-03,\n",
            "        -1.6790e-03, -1.0125e-02, -4.5538e-03,  1.7020e-03,  9.6378e-04,\n",
            "        -7.1739e-03,  3.9215e-03,  1.4459e-02,  6.2616e-03, -8.7273e-03,\n",
            "         4.5909e-04])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0203, -0.0186,  0.0056,  ..., -0.0510,  0.0320, -0.0560],\n",
            "        [ 0.0340, -0.0551,  0.0036,  ...,  0.0253, -0.0105, -0.0201],\n",
            "        [ 0.0033,  0.0194,  0.0108,  ...,  0.0152,  0.0104,  0.0178],\n",
            "        ...,\n",
            "        [ 0.0023, -0.0294,  0.0335,  ...,  0.0557, -0.0519,  0.0135],\n",
            "        [ 0.0110,  0.0237,  0.0595,  ...,  0.0158,  0.0176,  0.0488],\n",
            "        [ 0.0040,  0.0437, -0.0269,  ..., -0.0124,  0.0492, -0.0440]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-0.0047, -0.0048, -0.0035,  ..., -0.0047, -0.0030, -0.0025],\n",
            "        [-0.0072, -0.0074, -0.0052,  ..., -0.0071, -0.0045, -0.0037],\n",
            "        [-0.0034, -0.0034, -0.0025,  ..., -0.0033, -0.0021, -0.0017],\n",
            "        ...,\n",
            "        [ 0.0027,  0.0027,  0.0019,  ...,  0.0027,  0.0017,  0.0014],\n",
            "        [-0.0012, -0.0013, -0.0010,  ..., -0.0012, -0.0008, -0.0006],\n",
            "        [-0.0059, -0.0060, -0.0043,  ..., -0.0058, -0.0037, -0.0030]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([-2.1269e-03,  3.1970e-02, -5.6560e-02, -5.8539e-02,  8.1383e-05,\n",
            "         4.6225e-02, -3.0385e-02,  6.0508e-02,  1.8743e-02,  3.9250e-02,\n",
            "         4.0424e-02, -6.1973e-02, -3.3906e-02,  3.8972e-02, -3.3935e-02,\n",
            "        -4.4515e-02, -5.8807e-02, -1.2406e-02, -1.5181e-02, -1.0195e-02,\n",
            "        -1.7681e-02,  5.9681e-02, -3.0544e-02,  1.9533e-02,  1.8930e-02,\n",
            "        -3.1955e-02,  3.2695e-02, -5.8479e-02,  2.5840e-02,  4.8020e-02,\n",
            "        -3.4365e-03, -6.0529e-02,  1.0925e-02, -4.3483e-02,  5.0947e-02,\n",
            "        -4.9700e-02, -4.8567e-02, -6.1192e-02, -4.8245e-02, -5.2309e-02,\n",
            "        -3.8639e-02, -2.5967e-02, -2.1331e-02, -9.9144e-03, -4.0675e-02,\n",
            "         1.1755e-02,  3.2868e-02, -5.4521e-03, -7.9721e-03, -2.9543e-02,\n",
            "        -4.2363e-02, -2.6034e-02, -1.6983e-02,  4.8103e-02,  1.0279e-03,\n",
            "         1.5949e-02,  9.9677e-03, -3.1325e-02, -2.3612e-02, -4.3933e-03,\n",
            "        -3.7632e-02, -3.0434e-02,  9.1093e-03, -1.4874e-02],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-0.0355, -0.0534, -0.0252, -0.0048,  0.0604,  0.0340, -0.0612,  0.0436,\n",
            "         0.0074,  0.0461, -0.0313,  0.0417,  0.0605,  0.0258, -0.0193, -0.0275,\n",
            "         0.0299, -0.0384, -0.0303, -0.0235,  0.0175,  0.0391, -0.0073,  0.0040,\n",
            "        -0.0440,  0.0439,  0.0183, -0.0133, -0.0219, -0.0004,  0.0262,  0.0572,\n",
            "        -0.0086,  0.0440, -0.0690,  0.0538,  0.0282, -0.0204, -0.0512, -0.0605,\n",
            "         0.0275, -0.0555,  0.0158,  0.0554,  0.0088,  0.0368,  0.0062, -0.0325,\n",
            "         0.0513,  0.0055, -0.0540, -0.0089,  0.0444,  0.0104, -0.0370,  0.0418,\n",
            "         0.0073, -0.0571,  0.0331,  0.0305,  0.0379,  0.0198, -0.0091, -0.0437])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
            "grad:\n",
            "tensor([-4.7034e-04, -1.0434e-03, -5.5509e-04,  7.2076e-05,  3.6900e-04,\n",
            "        -7.5192e-04, -1.2924e-03,  1.1240e-03, -8.4940e-04, -1.0153e-03,\n",
            "        -7.1333e-04, -4.1889e-04, -5.0855e-04,  4.7090e-05, -1.1986e-03,\n",
            "        -1.6166e-03,  7.6998e-04, -2.1583e-03, -1.7030e-04, -4.7195e-04,\n",
            "        -1.0728e-03,  3.1624e-03, -2.4340e-04,  1.0685e-04, -9.1887e-04,\n",
            "        -5.3453e-04, -7.5709e-04, -9.6527e-04, -1.5688e-03, -1.3042e-03,\n",
            "        -2.0033e-04, -5.2788e-04, -1.1959e-04, -2.1535e-03, -5.6843e-04,\n",
            "        -9.8790e-04, -1.2281e-03,  2.8478e-04, -1.3219e-03, -2.0217e-04,\n",
            "        -1.2121e-03,  1.6841e-04, -4.5117e-04,  2.2560e-04, -1.0157e-03,\n",
            "         7.6856e-04, -1.3973e-04, -2.6369e-03,  1.0137e-03, -1.1963e-03,\n",
            "        -1.4458e-03, -1.1721e-03, -1.1336e-03,  2.8101e-04, -1.6744e-03,\n",
            "        -1.1839e-03, -1.2467e-03, -2.0650e-04, -1.3430e-03, -6.1294e-04,\n",
            "        -6.3473e-04,  5.6813e-04, -9.2039e-06,  2.8498e-04])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([ 2.8314e-04, -3.0455e-03,  3.2060e-03, -7.0520e-03, -1.5719e-02,\n",
            "         3.8342e-03, -5.4695e-03,  3.3875e-03, -1.0847e-02, -2.0097e-03,\n",
            "        -1.8673e-03,  2.9862e-03,  1.0051e-03,  8.5765e-03, -3.2508e-03,\n",
            "         1.2856e-02, -3.5616e-03, -7.9270e-03, -6.1117e-03,  1.6626e-04,\n",
            "         2.2826e-04, -1.5143e-02, -2.1178e-03, -1.8275e-03,  5.6709e-03,\n",
            "        -8.6332e-03, -9.8576e-03, -7.6563e-03, -8.1620e-03, -2.9993e-03,\n",
            "         1.5630e-03, -7.6260e-03, -5.5708e-03, -7.3477e-03,  3.1366e-03,\n",
            "        -2.3353e-03, -6.4150e-03,  1.5131e-03,  1.8343e-03, -2.5449e-03,\n",
            "         5.0027e-03, -5.9114e-03, -4.1913e-03, -3.7270e-03,  1.9189e-03,\n",
            "         9.3670e-03, -1.0317e-03, -8.9425e-03, -1.1599e-02, -5.0813e-03,\n",
            "        -5.8168e-03,  1.7269e-03, -9.2974e-04,  1.2672e-03, -8.1328e-03,\n",
            "        -5.5107e-03, -1.1155e-02,  2.9517e-03, -1.8929e-03, -5.2173e-05,\n",
            "        -6.2108e-04,  6.1646e-03,  6.6085e-03, -5.3485e-03])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[-0.0571, -0.0024, -0.0625,  ...,  0.0559, -0.1146,  0.0521],\n",
            "        [-0.0828,  0.1104, -0.0844,  ..., -0.0448, -0.0423, -0.0440],\n",
            "        [ 0.1076, -0.1101, -0.0570,  ...,  0.0809, -0.1022,  0.0916],\n",
            "        ...,\n",
            "        [-0.0540, -0.0023, -0.1121,  ..., -0.0803,  0.1079, -0.0656],\n",
            "        [ 0.0886, -0.0426,  0.1177,  ..., -0.0343, -0.0578, -0.1160],\n",
            "        [-0.0683,  0.0843, -0.0572,  ..., -0.0071,  0.0013, -0.0360]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 6.5480e-06,  2.7427e-05,  1.5738e-05,  ..., -7.6097e-06,\n",
            "         -1.6122e-06, -9.6201e-06],\n",
            "        [-4.2364e-06, -2.3557e-05, -5.5640e-07,  ..., -7.7119e-06,\n",
            "          1.0573e-05, -7.3264e-07],\n",
            "        [ 1.0483e-05,  2.4113e-05,  7.3453e-06,  ..., -7.9588e-07,\n",
            "         -9.2011e-06,  3.8211e-06],\n",
            "        ...,\n",
            "        [-4.0767e-06, -5.5566e-05, -1.4421e-05,  ...,  1.3885e-05,\n",
            "          1.6071e-06,  4.5312e-06],\n",
            "        [-1.4815e-05, -4.8800e-05, -2.5031e-05,  ...,  1.3406e-05,\n",
            "          6.5196e-06,  7.9154e-06],\n",
            "        [-1.0064e-05, -5.2186e-05, -1.2302e-05,  ..., -5.1693e-07,\n",
            "          1.5037e-05,  7.0053e-06]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0803,  0.0770, -0.1012,  ..., -0.1009, -0.0813,  0.0262],\n",
            "        [-0.0579,  0.0726, -0.0109,  ..., -0.0738,  0.0074, -0.0006],\n",
            "        [-0.0410,  0.0490, -0.1065,  ...,  0.1005,  0.0617,  0.0407],\n",
            "        ...,\n",
            "        [ 0.0874,  0.0847, -0.0761,  ..., -0.0621,  0.0311, -0.0415],\n",
            "        [ 0.1233, -0.1092,  0.1193,  ..., -0.0843,  0.0778,  0.0513],\n",
            "        [ 0.0314, -0.0704,  0.0647,  ..., -0.0888, -0.0286,  0.0861]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 9.8028e-06,  7.9922e-07, -1.4988e-05,  ..., -1.7818e-05,\n",
            "          2.2983e-05,  3.7082e-05],\n",
            "        [ 7.5988e-06, -1.7697e-05, -6.7036e-06,  ..., -1.8733e-05,\n",
            "          2.4228e-05,  3.9736e-05],\n",
            "        [ 6.2416e-06, -5.4257e-06,  4.7693e-06,  ..., -1.5580e-06,\n",
            "          6.9165e-06, -8.5762e-06],\n",
            "        ...,\n",
            "        [ 8.5002e-04,  3.4734e-03,  9.4541e-04,  ..., -5.0943e-04,\n",
            "         -9.3347e-04, -1.4452e-04],\n",
            "        [-4.5729e-04, -1.8635e-03, -5.1053e-04,  ...,  2.7194e-04,\n",
            "          5.0898e-04,  7.6355e-05],\n",
            "        [ 9.4965e-04,  3.8581e-03,  1.0547e-03,  ..., -5.6940e-04,\n",
            "         -1.0275e-03, -1.5336e-04]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 4.3631e-02,  7.2515e-02, -7.8827e-02,  ...,  5.2258e-02,\n",
            "         -1.1321e-01, -3.1564e-02],\n",
            "        [-9.9354e-02,  6.3428e-02, -6.4471e-02,  ..., -3.8252e-02,\n",
            "          7.9468e-05, -2.2488e-02],\n",
            "        [ 8.4034e-02, -2.0393e-02,  7.6522e-02,  ...,  6.7307e-02,\n",
            "          3.6185e-02,  6.3354e-02],\n",
            "        ...,\n",
            "        [ 6.8571e-02,  7.3973e-02,  1.0585e-01,  ..., -9.0609e-02,\n",
            "         -8.8043e-02, -2.2208e-02],\n",
            "        [ 1.0897e-02, -9.9220e-02,  1.0823e-01,  ..., -8.6764e-02,\n",
            "          5.6190e-02,  8.1889e-02],\n",
            "        [ 2.9524e-02,  7.4164e-02, -8.6951e-02,  ..., -3.7877e-03,\n",
            "          1.2317e-01,  4.2799e-02]], requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 5.8588e-04,  4.9115e-04, -1.2892e-03,  ..., -4.8386e-04,\n",
            "          1.3737e-03,  4.2971e-04],\n",
            "        [ 1.5094e-03,  1.2283e-03, -3.3144e-03,  ..., -1.2189e-03,\n",
            "          3.4803e-03,  1.1200e-03],\n",
            "        [-1.5037e-04, -1.2612e-04,  3.4510e-04,  ...,  1.2568e-04,\n",
            "         -3.4974e-04, -1.1194e-04],\n",
            "        ...,\n",
            "        [-6.4238e-04, -5.1778e-04,  1.4073e-03,  ...,  5.1190e-04,\n",
            "         -1.4735e-03, -4.9007e-04],\n",
            "        [ 4.7300e-05,  3.6245e-05, -1.0959e-04,  ..., -3.6886e-05,\n",
            "          1.2634e-04,  1.8021e-05],\n",
            "        [ 8.5533e-04,  7.0371e-04, -1.9024e-03,  ..., -7.0378e-04,\n",
            "          1.9997e-03,  6.4955e-04]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([ 0.0216,  0.0350,  0.0386, -0.1131,  0.0243,  0.1047, -0.0546, -0.0597,\n",
            "         0.1201, -0.1220,  0.0397,  0.0238, -0.0510, -0.0719, -0.1087,  0.0562,\n",
            "        -0.0886, -0.0136, -0.0908,  0.0090, -0.0694,  0.0221, -0.1048,  0.0418,\n",
            "        -0.1009,  0.0950, -0.0359,  0.1231,  0.0627, -0.0329,  0.0051, -0.0631,\n",
            "        -0.0483,  0.1045,  0.0230, -0.0788,  0.0339,  0.1115,  0.1199, -0.0582,\n",
            "        -0.1041,  0.1127,  0.0161, -0.0595,  0.0093,  0.0171,  0.0381, -0.0335,\n",
            "         0.0157,  0.0565, -0.0833, -0.0318,  0.0700,  0.0375, -0.0395, -0.0843,\n",
            "        -0.0706, -0.0625, -0.0411,  0.0319, -0.0594, -0.0799,  0.1089,  0.0731],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-0.0263, -0.0673,  0.0069, -0.0122,  0.0634,  0.0138, -0.0606,  0.0305,\n",
            "         0.0339,  0.0180, -0.0322,  0.0424,  0.0428,  0.0551, -0.0075, -0.0516,\n",
            "         0.0273, -0.0327, -0.0301, -0.0080,  0.0292,  0.0397,  0.0017, -0.0130,\n",
            "        -0.0455,  0.0331,  0.0131, -0.0036, -0.0159, -0.0097,  0.0213,  0.0550,\n",
            "        -0.0007,  0.0419, -0.0730,  0.0544,  0.0439, -0.0215, -0.0350, -0.0523,\n",
            "         0.0077, -0.0608,  0.0245,  0.0577, -0.0116,  0.0201, -0.0078, -0.0154,\n",
            "         0.0450,  0.0339, -0.0569,  0.0198,  0.0544,  0.0074, -0.0444,  0.0358,\n",
            "        -0.0019, -0.0666,  0.0331,  0.0226,  0.0162,  0.0285, -0.0022, -0.0386])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
            "grad:\n",
            "tensor([-2.8216e-04,  1.0774e-03, -8.7499e-04, -6.1951e-04, -1.5533e-04,\n",
            "        -1.3857e-03,  2.7407e-05,  9.1689e-04, -8.3714e-04, -1.7970e-03,\n",
            "         3.9456e-04,  6.3546e-04, -1.5299e-03, -3.5474e-03,  5.4516e-04,\n",
            "        -1.3975e-03, -1.0776e-04, -2.6739e-04,  3.4586e-04,  2.4235e-03,\n",
            "         2.7934e-03,  2.9390e-04,  1.1775e-04, -1.0707e-03,  1.8326e-03,\n",
            "        -6.1976e-04,  4.1839e-04, -1.4549e-04,  5.1059e-05,  2.4332e-04,\n",
            "         8.7206e-04, -6.2907e-05,  3.2203e-04, -1.2882e-04,  1.1930e-04,\n",
            "        -1.2917e-05,  1.6937e-03,  1.1336e-03, -2.3524e-04,  8.5602e-04,\n",
            "        -1.1345e-04, -1.0392e-05,  9.8027e-04, -3.8870e-05, -5.6930e-04,\n",
            "         8.4421e-05,  7.8211e-04, -1.2913e-03, -7.3955e-04, -2.6447e-03,\n",
            "         1.3045e-04,  1.2402e-04, -5.8686e-04,  7.2610e-04, -5.8196e-04,\n",
            "        -7.5907e-06,  1.1244e-03, -1.3279e-04, -4.0949e-05,  9.5547e-05,\n",
            "        -5.2106e-04,  8.8847e-05, -1.9769e-06,  2.8377e-05])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-6.9137e-03,  7.8638e-03, -2.4103e-02,  9.9156e-03,  3.4511e-03,\n",
            "         1.4290e-02, -1.8736e-04,  4.6456e-03, -7.5852e-03,  1.9193e-02,\n",
            "         5.3052e-03, -4.6614e-03,  1.8687e-02, -2.1720e-02, -1.2845e-02,\n",
            "         2.1107e-02,  1.7347e-03, -1.4349e-03,  3.5853e-03, -1.4990e-02,\n",
            "        -1.5792e-02,  5.6560e-03, -7.2098e-03,  2.1078e-02,  1.1184e-02,\n",
            "         1.5941e-02,  1.0144e-02, -5.3441e-03,  4.3150e-04,  5.4861e-03,\n",
            "         5.5530e-03,  2.1648e-03, -3.1637e-03, -8.2808e-04, -7.0580e-03,\n",
            "        -3.6523e-03, -9.7626e-03, -6.9731e-03, -4.7659e-03, -7.6182e-03,\n",
            "         7.3650e-03, -7.8097e-04, -5.7246e-03,  2.1619e-04,  1.0787e-02,\n",
            "         1.0471e-02,  4.0132e-03, -1.2733e-02,  4.8595e-03, -2.9415e-02,\n",
            "        -1.0218e-03, -2.2688e-02, -1.4620e-02,  8.1430e-03,  9.6917e-03,\n",
            "        -2.6996e-04,  5.1577e-03,  2.0662e-03,  6.3459e-06, -1.5235e-03,\n",
            "         8.4252e-03, -2.6640e-03, -9.0304e-04, -7.7817e-03])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0661,  0.0567, -0.0457,  ...,  0.0714, -0.0266, -0.0813],\n",
            "        [ 0.0878,  0.1122,  0.0034,  ...,  0.0310, -0.0207, -0.0398],\n",
            "        [-0.0620,  0.0461,  0.0330,  ...,  0.0115, -0.0247,  0.0851],\n",
            "        ...,\n",
            "        [-0.0354, -0.1139,  0.0212,  ..., -0.1048, -0.0537,  0.1135],\n",
            "        [-0.0856, -0.0738, -0.0418,  ...,  0.0531, -0.0750, -0.0797],\n",
            "        [ 0.0269,  0.1046, -0.0285,  ...,  0.0084,  0.1119, -0.1162]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-4.2577e-05, -3.9113e-04,  2.5059e-04,  ...,  8.1668e-05,\n",
            "         -6.0827e-04,  4.3007e-04],\n",
            "        [-7.4739e-05, -2.2971e-03,  8.4065e-05,  ...,  5.0181e-04,\n",
            "         -2.0035e-03,  6.7890e-04],\n",
            "        [-1.1881e-03,  1.8246e-03,  3.1212e-04,  ..., -8.1197e-04,\n",
            "          1.9545e-03,  3.9028e-04],\n",
            "        ...,\n",
            "        [ 2.5893e-05, -3.1548e-05,  1.2478e-05,  ...,  8.2366e-05,\n",
            "         -4.6651e-05, -3.4710e-05],\n",
            "        [ 3.4748e-04, -2.8089e-04,  1.9573e-05,  ...,  2.2986e-04,\n",
            "         -3.1121e-04,  5.6078e-04],\n",
            "        [-8.1830e-06,  8.9832e-04, -2.6280e-04,  ..., -5.8561e-04,\n",
            "          1.1672e-03, -3.8363e-04]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([ 0.0222,  0.0712, -0.0729,  0.1073, -0.0646, -0.0111,  0.0250,  0.0167,\n",
            "         0.0044,  0.0680,  0.1087,  0.1053, -0.0551,  0.1154,  0.0635,  0.0744,\n",
            "         0.0992,  0.1016, -0.0036,  0.0246, -0.0251, -0.1099,  0.0053, -0.0300,\n",
            "         0.1176, -0.0032,  0.0098, -0.0483, -0.0184,  0.0756,  0.0251, -0.1211,\n",
            "         0.1205, -0.0941, -0.0277,  0.1177, -0.1166,  0.0052, -0.1104,  0.0695,\n",
            "        -0.0465, -0.0753,  0.0794,  0.0289, -0.0379,  0.0608, -0.0064, -0.0596,\n",
            "        -0.0795,  0.0710,  0.0056,  0.0547, -0.0532, -0.0362,  0.0094,  0.0916,\n",
            "         0.0327, -0.0364,  0.1224,  0.0979, -0.0193, -0.0643,  0.0818,  0.0147,\n",
            "        -0.0456,  0.0910,  0.0582, -0.0162,  0.0131,  0.0396, -0.0708,  0.0462,\n",
            "        -0.0219,  0.0579, -0.0163,  0.0024,  0.1058, -0.0164, -0.0009,  0.0493,\n",
            "         0.0570, -0.0986, -0.0625,  0.0747, -0.0688,  0.1112, -0.0809, -0.0412,\n",
            "        -0.0429, -0.0204,  0.0258,  0.0211,  0.0078,  0.0510, -0.1087,  0.0799,\n",
            "        -0.0238, -0.0045,  0.1230,  0.0563,  0.0461,  0.0588, -0.0553, -0.0282,\n",
            "         0.0540,  0.0071, -0.0057,  0.0135,  0.0325, -0.0927, -0.1190,  0.0207,\n",
            "        -0.0971,  0.0626, -0.0298,  0.1103,  0.0936,  0.0696, -0.0276, -0.1190,\n",
            "         0.0579, -0.0992,  0.0596, -0.1211, -0.1058,  0.0852,  0.0093,  0.1120,\n",
            "        -0.1240,  0.0358,  0.0291, -0.1110, -0.0906,  0.0551, -0.0465,  0.0541,\n",
            "        -0.0366, -0.0554, -0.1095, -0.0489,  0.0268, -0.0210,  0.1080, -0.0406,\n",
            "        -0.1138, -0.0244, -0.0741,  0.0635, -0.0826, -0.0142,  0.0534,  0.1129,\n",
            "         0.0656, -0.0740, -0.0095, -0.0574,  0.0092, -0.0685,  0.0872,  0.0243,\n",
            "        -0.0013, -0.0373,  0.0288,  0.0940,  0.0665,  0.0502,  0.0070, -0.0945,\n",
            "        -0.0856, -0.0099,  0.0428, -0.0875, -0.0937,  0.0719, -0.1176, -0.0005,\n",
            "        -0.0631, -0.1194,  0.0650, -0.0728,  0.0231, -0.0924, -0.0022,  0.0163,\n",
            "        -0.0532, -0.0589,  0.0425,  0.0628,  0.0926, -0.0100,  0.0831, -0.1019,\n",
            "         0.1211, -0.0800,  0.0493,  0.0188,  0.0847,  0.0795, -0.0069,  0.0540,\n",
            "         0.0118,  0.0844,  0.0003,  0.0100,  0.0793,  0.0451,  0.0797,  0.0982,\n",
            "         0.0314,  0.1166,  0.0079, -0.1055, -0.1242, -0.1103, -0.1037, -0.0903,\n",
            "         0.0382, -0.0709,  0.0085,  0.0077, -0.0799, -0.0827,  0.0930, -0.0673,\n",
            "         0.0755,  0.0921,  0.0135,  0.0313, -0.1146, -0.0674, -0.0671, -0.1086,\n",
            "        -0.0187, -0.0144,  0.0640, -0.0411,  0.0615,  0.0200,  0.0880, -0.1023,\n",
            "        -0.0990,  0.0336,  0.0463, -0.0125, -0.0697,  0.0046, -0.0898, -0.0527,\n",
            "         0.0307, -0.0491,  0.0245,  0.0694, -0.0964,  0.1237,  0.0907, -0.0049],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-2.6726e-03, -9.1912e-03,  8.8278e-03, -1.3831e-03, -8.5177e-03,\n",
            "         8.3271e-04,  2.7184e-03, -5.9475e-03, -2.9330e-03,  5.0488e-03,\n",
            "         2.4656e-03, -5.1201e-03, -1.9364e-03, -6.1330e-03,  7.6883e-03,\n",
            "         7.8857e-03, -7.2561e-03,  1.7196e-03, -6.7499e-03,  3.7570e-04,\n",
            "         3.9035e-04,  6.6543e-03, -4.0413e-05, -5.0939e-03, -4.7858e-03,\n",
            "         2.6578e-03, -5.0458e-04, -3.1908e-04,  9.9124e-03, -2.1783e-03,\n",
            "        -4.0717e-03,  4.3695e-03, -2.8443e-03,  6.9412e-05, -1.0763e-03,\n",
            "        -1.2649e-02, -1.3267e-03, -4.4514e-03,  6.7802e-03, -1.3496e-03,\n",
            "        -1.3906e-03, -5.2040e-03,  2.5923e-03, -8.0528e-04, -3.1183e-03,\n",
            "         1.3880e-02,  5.1805e-03, -1.1735e-03, -4.6760e-03,  7.4273e-03,\n",
            "         4.4357e-03,  1.1593e-03,  1.0078e-02, -1.5619e-03, -7.5502e-04,\n",
            "        -6.4249e-03,  1.7497e-03,  5.8299e-03,  5.2569e-03, -9.2243e-04,\n",
            "         3.0848e-03, -2.3842e-03,  1.4014e-02, -6.1805e-03,  8.5886e-03,\n",
            "         5.1258e-03,  2.5796e-03,  3.6811e-03,  1.8960e-03,  9.4407e-03,\n",
            "        -7.9122e-04,  6.4444e-04,  3.2347e-03,  4.2065e-03,  9.3764e-03,\n",
            "        -2.0817e-03, -1.5612e-03, -1.2301e-05, -9.1139e-03,  4.7031e-03,\n",
            "         6.2183e-03, -6.1587e-03,  2.1810e-03, -4.6571e-03, -1.9771e-03,\n",
            "        -7.5055e-03,  1.1490e-03,  8.8839e-04, -3.5698e-04, -2.5829e-03,\n",
            "         2.5238e-03,  1.5286e-03, -2.3137e-03,  5.7027e-03,  5.6077e-03,\n",
            "        -5.8395e-03,  1.0293e-02,  2.3825e-04,  7.2890e-03, -1.0064e-02,\n",
            "         8.0091e-03, -3.4872e-03,  2.8628e-03,  4.0230e-03,  7.7422e-03,\n",
            "         6.6911e-03,  9.3079e-04, -3.7507e-03, -7.6174e-03,  5.0447e-03,\n",
            "        -2.5477e-03, -3.3852e-03,  8.2487e-04, -1.8282e-03,  5.5787e-04,\n",
            "        -4.5605e-03,  2.8429e-03,  3.1197e-03,  1.2263e-02,  9.5598e-03,\n",
            "        -1.4500e-03, -1.6897e-03, -3.8280e-03, -2.3692e-03, -6.0682e-03,\n",
            "         6.7534e-03,  1.0402e-02, -2.0390e-03, -3.9244e-03, -4.5631e-03,\n",
            "        -3.0392e-04, -9.6950e-03,  5.7610e-03,  3.9451e-03, -6.1062e-03,\n",
            "         3.8622e-03, -4.4253e-03,  7.1478e-04, -2.9133e-03, -5.5307e-04,\n",
            "        -5.6953e-03, -1.0335e-02,  1.0574e-02,  8.4472e-04, -1.9221e-04,\n",
            "         4.3309e-03,  8.2713e-03, -1.9732e-03, -2.6593e-03,  9.7447e-03,\n",
            "        -1.3114e-02, -4.0791e-03, -1.2481e-02, -2.8787e-03, -2.2536e-03,\n",
            "         9.6819e-04, -4.3443e-03,  6.8023e-03,  5.3221e-03,  2.5449e-03,\n",
            "         3.5844e-03,  5.8994e-03,  1.6365e-03, -1.1626e-03,  1.8270e-03,\n",
            "        -2.8139e-03,  5.1113e-04,  3.0565e-03, -3.1284e-04,  3.3037e-03,\n",
            "         1.3932e-02,  1.4209e-03, -1.7278e-02,  1.3319e-03, -5.1718e-03,\n",
            "         8.9596e-03, -1.2628e-03, -7.2296e-03,  1.3880e-02,  1.8043e-03,\n",
            "         1.1007e-03,  2.7950e-04,  4.1826e-03,  1.5773e-02, -2.8289e-03,\n",
            "        -1.8271e-03,  2.4260e-03,  2.5284e-05, -4.7468e-03,  3.1321e-03,\n",
            "         5.7745e-03, -3.0989e-03, -1.1763e-03,  5.8999e-03,  1.4076e-03,\n",
            "         4.7990e-03, -3.5666e-03,  2.7826e-03,  4.1665e-03,  4.9485e-03,\n",
            "         2.5181e-03,  3.4515e-03, -3.0995e-03,  2.0271e-03,  9.6672e-03,\n",
            "        -5.1451e-04, -1.9321e-03,  3.0419e-03, -6.2844e-04,  9.2751e-04,\n",
            "         8.8376e-03,  2.8548e-03, -3.1718e-03, -6.1614e-03, -7.1451e-03,\n",
            "         2.4758e-03,  1.4068e-02, -8.3316e-04,  2.2359e-03, -1.0885e-03,\n",
            "        -1.9994e-03, -6.0556e-03, -9.8392e-03, -2.7792e-03, -5.1515e-04,\n",
            "         8.3226e-03,  6.5505e-03,  7.6543e-03, -3.6888e-04,  1.6588e-03,\n",
            "        -9.6602e-04,  3.8756e-03,  9.1787e-04, -1.2163e-02, -1.3266e-03,\n",
            "        -3.9907e-04,  4.3691e-03,  7.5552e-03, -1.8805e-03,  2.8512e-03,\n",
            "         4.9062e-03, -3.7945e-03, -6.0359e-03,  4.4340e-03, -5.5097e-03,\n",
            "        -4.4554e-03,  3.9795e-04, -8.2340e-03, -3.4456e-03, -1.0488e-02,\n",
            "         2.7500e-03, -3.5598e-03,  1.4645e-03, -4.5969e-04, -3.2050e-03,\n",
            "         3.0172e-03])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[ 0.0290,  0.0505, -0.0254,  ..., -0.0074, -0.0211,  0.0536],\n",
            "        [-0.0246,  0.0361,  0.0187,  ...,  0.0075, -0.0542,  0.0166],\n",
            "        [-0.0502, -0.0508, -0.0519,  ..., -0.0602,  0.0443,  0.0433],\n",
            "        ...,\n",
            "        [-0.0233,  0.0214,  0.0290,  ...,  0.0271, -0.0467,  0.0335],\n",
            "        [ 0.0102,  0.0305, -0.0517,  ...,  0.0218,  0.0384,  0.0171],\n",
            "        [ 0.0444,  0.0436, -0.0160,  ...,  0.0267,  0.0461, -0.0216]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[-0.0037, -0.0049, -0.0035,  ..., -0.0072, -0.0030, -0.0041],\n",
            "        [-0.0078, -0.0103, -0.0074,  ..., -0.0153, -0.0065, -0.0088],\n",
            "        [-0.0002, -0.0003, -0.0002,  ..., -0.0004, -0.0002, -0.0002],\n",
            "        ...,\n",
            "        [ 0.0028,  0.0038,  0.0027,  ...,  0.0056,  0.0024,  0.0032],\n",
            "        [-0.0011, -0.0014, -0.0010,  ..., -0.0021, -0.0009, -0.0012],\n",
            "        [-0.0046, -0.0062, -0.0044,  ..., -0.0091, -0.0039, -0.0052]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([ 0.0296,  0.0297, -0.0222, -0.0257, -0.0189,  0.0109, -0.0327, -0.0288,\n",
            "        -0.0334, -0.0449,  0.0427,  0.0212,  0.0568, -0.0204,  0.0563, -0.0135,\n",
            "        -0.0134, -0.0255,  0.0437,  0.0064, -0.0523, -0.0176,  0.0021, -0.0339,\n",
            "         0.0247, -0.0073, -0.0232,  0.0181,  0.0036, -0.0449, -0.0133,  0.0459,\n",
            "         0.0085,  0.0124, -0.0063, -0.0539,  0.0280,  0.0410, -0.0224,  0.0261,\n",
            "         0.0535,  0.0446,  0.0279, -0.0064,  0.0076,  0.0517, -0.0534,  0.0330,\n",
            "        -0.0524,  0.0480, -0.0059,  0.0175,  0.0294,  0.0301, -0.0573,  0.0348,\n",
            "        -0.0421, -0.0365, -0.0248, -0.0496, -0.0407, -0.0213, -0.0510, -0.0075],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([-0.0288, -0.0612, -0.0017, -0.0147,  0.0570,  0.0198, -0.0612,  0.0388,\n",
            "         0.0148,  0.0271, -0.0365,  0.0464,  0.0421,  0.0470, -0.0068, -0.0484,\n",
            "         0.0281, -0.0372, -0.0340, -0.0088,  0.0328,  0.0334, -0.0005, -0.0167,\n",
            "        -0.0549,  0.0280,  0.0084, -0.0084, -0.0225, -0.0060,  0.0205,  0.0549,\n",
            "        -0.0057,  0.0447, -0.0623,  0.0574,  0.0377, -0.0138, -0.0468, -0.0532,\n",
            "         0.0201, -0.0548,  0.0213,  0.0551, -0.0020,  0.0266,  0.0021, -0.0201,\n",
            "         0.0464,  0.0342, -0.0533,  0.0136,  0.0585,  0.0025, -0.0467,  0.0419,\n",
            "         0.0023, -0.0594,  0.0329,  0.0320,  0.0295,  0.0223, -0.0083, -0.0365])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
            "grad:\n",
            "tensor([ 1.9472e-04,  9.9611e-05,  5.4463e-04,  6.3435e-04,  1.1492e-03,\n",
            "         7.3198e-04,  8.0502e-04, -1.6769e-03,  2.8206e-03,  1.7143e-03,\n",
            "         9.3102e-04,  8.4581e-04,  6.4707e-04,  1.5756e-03,  1.4826e-03,\n",
            "         3.8534e-04,  7.9946e-04,  2.5427e-04,  8.4207e-05,  1.0179e-03,\n",
            "         2.0373e-03,  4.7023e-04, -1.4927e-04,  1.0322e-03, -5.3475e-04,\n",
            "         1.2683e-03, -1.9057e-04,  2.6873e-04,  2.2107e-03,  3.9755e-04,\n",
            "         6.9335e-04,  1.0439e-03,  1.0259e-03, -9.1397e-04, -1.2983e-03,\n",
            "        -4.0548e-04,  8.7609e-04, -5.0649e-04,  1.7520e-03,  4.2350e-04,\n",
            "         2.8087e-03, -9.6576e-06,  1.1563e-03, -6.8820e-04,  2.1323e-03,\n",
            "        -4.3277e-04,  1.5683e-03,  1.2009e-03,  1.0127e-04,  3.4490e-04,\n",
            "         1.1334e-03,  2.3853e-04, -1.8506e-04,  8.1998e-04,  1.0586e-03,\n",
            "         2.7261e-03,  7.7752e-04,  1.6348e-03, -9.4952e-05,  9.6502e-04,\n",
            "         3.0489e-03,  6.3097e-04,  1.8031e-03,  7.2991e-04])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([ 3.7666e-04, -8.2272e-03,  6.3799e-03,  2.7144e-04,  4.2442e-03,\n",
            "        -8.2240e-03, -1.6323e-03, -1.0316e-02,  1.7186e-02, -1.1469e-02,\n",
            "         2.1512e-03, -6.3085e-03, -1.4979e-03,  5.8996e-03, -2.9865e-03,\n",
            "        -5.3725e-03, -3.1144e-03,  2.5266e-03,  1.8483e-03, -1.3237e-03,\n",
            "        -5.8068e-03,  4.0250e-03, -7.3249e-05,  1.5950e-03,  7.2024e-03,\n",
            "         2.9455e-03,  2.5366e-03,  2.8533e-03,  4.7531e-03, -5.8830e-03,\n",
            "        -1.0671e-03, -1.8331e-03,  2.7954e-03, -4.9363e-03, -1.2909e-02,\n",
            "        -5.1039e-03,  4.0407e-03, -9.7861e-03,  9.7554e-03, -1.3721e-03,\n",
            "        -1.4781e-02, -8.2177e-03,  1.0667e-03,  3.3362e-04, -1.1937e-02,\n",
            "        -8.6877e-03, -1.2227e-02,  2.7082e-03, -3.6737e-03, -2.3737e-03,\n",
            "        -5.7542e-03,  3.9653e-03, -6.2992e-03,  2.7218e-03, -3.2544e-05,\n",
            "        -8.4957e-03, -6.4081e-03, -9.4160e-03, -1.8969e-03, -1.1674e-02,\n",
            "        -1.5749e-02,  4.1003e-03,  4.2202e-03, -4.4856e-03])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([[-0.0576, -0.1224, -0.0034, -0.0293,  0.1140,  0.0397, -0.1224,  0.0777,\n",
            "          0.0296,  0.0541, -0.0729,  0.0928,  0.0842,  0.0939, -0.0136, -0.0967,\n",
            "          0.0562, -0.0744, -0.0680, -0.0177,  0.0656,  0.0668, -0.0011, -0.0334,\n",
            "         -0.1097,  0.0561,  0.0168, -0.0168, -0.0450, -0.0120,  0.0411,  0.1098,\n",
            "         -0.0114,  0.0894, -0.1245,  0.1147,  0.0754, -0.0276, -0.0935, -0.1064,\n",
            "          0.0402, -0.1095,  0.0426,  0.1103, -0.0040,  0.0532,  0.0042, -0.0403,\n",
            "          0.0929,  0.0683, -0.1066,  0.0271,  0.1169,  0.0050, -0.0934,  0.0839,\n",
            "          0.0046, -0.1187,  0.0658,  0.0640,  0.0590,  0.0446, -0.0165, -0.0730]],\n",
            "       requires_grad=True)\n",
            "grad:\n",
            "tensor([[ 0.0437,  0.1741, -0.0061, -0.1012, -0.0338,  0.0582, -0.1549,  0.1799,\n",
            "          0.1336, -0.0854,  0.0982, -0.0196, -0.0558,  0.0820, -0.0377, -0.1072,\n",
            "         -0.1324,  0.1799,  0.1482, -0.1031, -0.2482, -0.0519, -0.1415, -0.0841,\n",
            "          0.0199,  0.0774,  0.0108,  0.1389,  0.1440,  0.0979,  0.1897, -0.0208,\n",
            "         -0.0580,  0.1368,  0.0048,  0.0207,  0.0315,  0.0160,  0.0247, -0.1809,\n",
            "         -0.0622,  0.1026, -0.1138, -0.0501, -0.0285,  0.0506,  0.0731,  0.1322,\n",
            "         -0.0439,  0.1385, -0.1067, -0.0363,  0.0961,  0.1321, -0.1935, -0.0928,\n",
            "          0.0172, -0.0590,  0.0714, -0.0831, -0.1378, -0.0107,  0.0940,  0.0061]])\n",
            "-----------------\n",
            "param:\n",
            "Parameter containing:\n",
            "tensor([0.0018], requires_grad=True)\n",
            "grad:\n",
            "tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3J_hnwaymkD"
      },
      "source": [
        "## Reversible Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWoEwNAFZyxa"
      },
      "source": [
        "class ReversibleSequence(nn.Module):\n",
        "    def __init__(self, blocks, layer_dropout = 0., reverse_thres = 0, send_signal = False):\n",
        "        super().__init__()\n",
        "        self.layer_dropout = layer_dropout\n",
        "        self.reverse_thres = reverse_thres # uses revblocks if seq_len else irrev_blocks\n",
        "\n",
        "        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n",
        "        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n",
        "\n",
        "    def forward(self, x, arg_route = (True, True), **kwargs):\n",
        "        reverse = x.shape[1] > self.reverse_thres\n",
        "        blocks = self.blocks if reverse else self.irrev_blocks\n",
        "\n",
        "        if self.training and self.layer_dropout > 0:\n",
        "            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) < self.layer_dropout\n",
        "            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]\n",
        "            blocks = self.blocks[:1] if len(blocks) == 0 else blocks\n",
        "\n",
        "        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n",
        "        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n",
        "\n",
        "        if not reverse:\n",
        "            for block in blocks:\n",
        "                x = block(x, **block_kwargs)\n",
        "            return x\n",
        "\n",
        "        return _ReversibleFunction.apply(x, blocks, block_kwargs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwoTMdxTnoal"
      },
      "source": [
        "bs = 8\n",
        "sl = 1024\n",
        "d_model = 64\n",
        "x = torch.randn(bs, sl, d_model)\n",
        "x2 = torch.cat([x, x], dim=-1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNKK6Iqmnoal"
      },
      "source": [
        "norm_wrapper = PreNorm\n",
        "blocks = []\n",
        "for i in range(2):\n",
        "    attn = Attention(d_model)\n",
        "    ff = FeedForward(d_model)\n",
        "    f = PreNorm(d_model, attn)\n",
        "    g = PreNorm(d_model, ff)\n",
        "    blocks.append(nn.ModuleList([f, g]))\n",
        "layers = ReversibleSequence(nn.ModuleList(blocks), )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw87t-wxnoam",
        "outputId": "6d49756e-9872-457d-c513-23b3f22314b2"
      },
      "source": [
        "out = layers(x2)\n",
        "out.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ_KDmYizcem"
      },
      "source": [
        "Gradients cannot be caluclated directly on ReversibleSequence as no params require grad at forward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5n1hIxtClrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc0621a-dae0-4661-e186-8a7eb4498a77"
      },
      "source": [
        "try:\n",
        "    out.mean().backward()\n",
        "except RuntimeError as e:\n",
        "    print(e)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "element 0 of tensors does not require grad and does not have a grad_fn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkK_S0A5E3sC"
      },
      "source": [
        "## Reformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0
        ],
        "id": "103KRr_nIegI"
      },
      "source": [
        "class ReformerEncoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 dim, \n",
        "                 depth, \n",
        "                 max_seq_len, \n",
        "                 heads = 8, \n",
        "                 dim_head = None, \n",
        "                 bucket_size = 64, \n",
        "                 n_hashes = 8, \n",
        "                 ff_chunks = 1, #100 to set reasonable default when tested\n",
        "                 attn_chunks = None, # ??\n",
        "                 causal = False, \n",
        "                 weight_tie = False, # ??\n",
        "                 attn_dropoup = 0.,\n",
        "                 post_attn_dropout = 0.,\n",
        "                 lsh_dropout = 0., \n",
        "                 ff_dropout = 0.,  \n",
        "                 ff_d = None, \n",
        "                 layer_dropout = 0., \n",
        "                 lsh_attend_across_buckets = True, \n",
        "                 lsh_allow_duplicate_attention = True, \n",
        "                 random_rotations_per_head = False,                  \n",
        "                 use_full_attn = False, \n",
        "                 full_attn_thres = 0, \n",
        "                 reverse_thres = 0,  \n",
        "                 one_value_head = False, \n",
        "                 n_local_attn_heads = 0,\n",
        "                 prenorm=True):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "\n",
        "        self.bucket_size = bucket_size\n",
        "        # self.full_attn_thres = full_attn_thres\n",
        "        \n",
        "        # use regular attention for now\n",
        "        get_attn = lambda: Attention(dim, heads, causal=causal, dropout=attn_dropoup)\n",
        "        # get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dim_head = dim_head, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n",
        "        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, d_ff=ff_d, dropout=ff_dropout), along_dim = -2)\n",
        "        get_ff = lambda: ChunkedFeedForward(dim, ff_d, )\n",
        "\n",
        "        # option to share weights between layers (ALBERT style)\n",
        "        # if weight_tie:\n",
        "        #     get_attn, get_ff, get_pkm = map(cache_fn, (get_attn, get_ff, get_pkm))\n",
        "\n",
        "        blocks = []\n",
        "        #TODO: find where ReZero proposed\n",
        "        #residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n",
        "        norm_wrapper = PreNorm if prenorm else PostNorm\n",
        "        \n",
        "        for ind in range(depth):\n",
        "            layer_num = ind + 1\n",
        "\n",
        "            attn = get_attn()\n",
        "            ff = get_ff()\n",
        "\n",
        "            f = norm_wrapper(dim, attn)\n",
        "            g = norm_wrapper(dim, ff)\n",
        "\n",
        "            blocks.append(nn.ModuleList([f, g]))\n",
        "        # send_signal is not implemented for now\n",
        "        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout=layer_dropout, reverse_thres=reverse_thres, send_signal=False)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = torch.cat([x, x], dim = -1)\n",
        "        arg_route = (True, False)\n",
        "        # pdb.set_trace()\n",
        "        x = self.layers(x, arg_route = arg_route, **kwargs)\n",
        "        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPBHcOdXrBNQ"
      },
      "source": [
        "bs = 8\n",
        "sl = 1024\n",
        "d_model = 64\n",
        "x = torch.randn(bs, sl, d_model)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imWH2fb6rBKc"
      },
      "source": [
        "module = ReformerEncoder(d_model, 2, sl)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7oSMuMaufek"
      },
      "source": [
        "# module"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jP25R5Rt5u8",
        "outputId": "48e3e20f-2942-49cd-c7eb-0fe52f67579f"
      },
      "source": [
        "out = module(x)\n",
        "out.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIchGQ8DYYdk"
      },
      "source": [
        "## ReformerLM test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF3DxPlNYZJE"
      },
      "source": [
        "class ReformerLM(nn.Module):#, TransformerLM):\n",
        "    def __init__(self,\n",
        "                 vocab_sz,\n",
        "                 dim, \n",
        "                 depth = 6,\n",
        "                 tie_weights = True,\n",
        "                 max_seq_len = 512, \n",
        "                 heads = 8, \n",
        "                 dim_head = None, \n",
        "                 bucket_size = 64, \n",
        "                 n_hashes = 8, \n",
        "                 ff_chunks = 100, \n",
        "                 attn_chunks = None, # ??\n",
        "                 causal = True, \n",
        "                 weight_tie = False, # ??\n",
        "                 attn_dropoup = 0.,\n",
        "                 post_attn_dropout = 0.,\n",
        "                 lsh_dropout = 0., \n",
        "                 ff_dropout = 0.,  \n",
        "                 ff_d = None, \n",
        "                 layer_dropout = 0., \n",
        "                 lsh_attend_across_buckets = True, \n",
        "                 lsh_allow_duplicate_attention = True, \n",
        "                 random_rotations_per_head = False,                  \n",
        "                 use_full_attn = False, \n",
        "                 full_attn_thres = 0, \n",
        "                 reverse_thres = 0,  \n",
        "                 one_value_head = False, \n",
        "                 n_local_attn_heads = 0,\n",
        "                 prenorm=True):\n",
        "        super().__init__()\n",
        "        self.emb = TransformerEmbedding(vocab_sz, dim, max_seq_len=max_seq_len)\n",
        "        #temp line to mark we need to route more args to encoder\n",
        "        kwargs = {'causal':True}\n",
        "        self.encoder = ReformerEncoder(dim, depth, max_seq_len, **kwargs)\n",
        "        self.proj = nn.Linear(d_model, vocab_sz)\n",
        "        if tie_weights: self.proj.weight = self.emb.emb.weight\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.emb(x)\n",
        "        x = self.encoder(x, mask=mask)\n",
        "        return self.proj(x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxnMgs8UbFi2",
        "outputId": "a8ca4aac-2d24-4e79-c375-c5149f1f4047"
      },
      "source": [
        "x = torch.randint(d_emb-1, (bs, sl))\n",
        "m = ReformerLM(d_emb, d_model, depth=2, max_seq_len=sl)\n",
        "out = m(x)\n",
        "out.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 1024, 256])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZQdhxn-bFgA"
      },
      "source": [
        "out.mean().backward()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGHciii0d5nH"
      },
      "source": [
        "### ReformerLM training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eRFTkwFJXD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c924814c-1cf8-496c-92c8-3227373eac16"
      },
      "source": [
        "!pip install -Uqq fastai"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▊                              | 10kB 35.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20kB 38.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30kB 21.2MB/s eta 0:00:01\r\u001b[K     |███████                         | 40kB 19.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 51kB 20.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71kB 15.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 81kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 92kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 102kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 122kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 133kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 143kB 15.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 153kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 163kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 174kB 15.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 184kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 194kB 15.6MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |██████▉                         | 10kB 32.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 20kB 35.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 30kB 38.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 40kB 41.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOEbl-UAEvdW"
      },
      "source": [
        "from fastai.text.all import *"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50zm2GZQEyX7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "5bfcf693-d5e7-4920-c8aa-5e802872fb27"
      },
      "source": [
        "path = untar_data(URLs.IMDB_SAMPLE)\n",
        "path.ls()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#1) [Path('/root/.fastai/data/imdb_sample/texts.csv')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwYbk7Hl5-0k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "d8ad8f5e-e48d-4d4c-ac8f-209bc5228c11"
      },
      "source": [
        "df = pd.read_csv(path/'texts.csv')\n",
        "df.head(2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>is_valid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  ... is_valid\n",
              "0  negative  ...    False\n",
              "1  positive  ...    False\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txosH2V05PM3"
      },
      "source": [
        "def add_eos(text):\n",
        "    return text + f' {EOS}'"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dZO6tcv5WWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "9e43acd8-e1f9-40d0-861f-c1f4695705fc"
      },
      "source": [
        "dls = DataBlock(blocks=TextBlock.from_df('text', is_lm=True, rules=[add_eos], seq_len=128),\n",
        "                get_x=ColReader('text'), splitter=RandomSplitter(0.1)).dataloaders(df, bs=16)\n",
        "\n",
        "dls.show_batch(max_n=2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The production quality , cast , premise , authentic New England ( xxunk , xxunk ? ) locale and lush John Williams score should have resulted in a 3 - 4 star collectors item . Unfortunately , all we got was a xxunk 2 star \" decent \" flick , mostly memorable for what it tried to do xxunk bring an art house style film xxunk . The small town locale and story of ordinary people is a genre to itself , and if well done , will satisfy most xxunk . Jane Fonda was unable to hide her xxunk enough to make her character believable . I wondered why she was n't doing a post xxunk at xxunk instead of working in a dead end factory job</td>\n",
              "      <td>production quality , cast , premise , authentic New England ( xxunk , xxunk ? ) locale and lush John Williams score should have resulted in a 3 - 4 star collectors item . Unfortunately , all we got was a xxunk 2 star \" decent \" flick , mostly memorable for what it tried to do xxunk bring an art house style film xxunk . The small town locale and story of ordinary people is a genre to itself , and if well done , will satisfy most xxunk . Jane Fonda was unable to hide her xxunk enough to make her character believable . I wondered why she was n't doing a post xxunk at xxunk instead of working in a dead end factory job in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is very much a part of the crap . I saw this film purely because I want to be able to say I 've seen everything on the DPP 's list ( just two more to go ! ) , and I 'm guessing that 's why most other people who have seen it , saw it . But if you 're not on the xxunk for xxunk ; there really is no reason to bother with this one . xxeos Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people . If I labor all my days and I can save but one soul from watching this movie , how great will be my</td>\n",
              "      <td>very much a part of the crap . I saw this film purely because I want to be able to say I 've seen everything on the DPP 's list ( just two more to go ! ) , and I 'm guessing that 's why most other people who have seen it , saw it . But if you 're not on the xxunk for xxunk ; there really is no reason to bother with this one . xxeos Every once in a long while a movie will come along that will be so awful that I feel compelled to warn people . If I labor all my days and I can save but one soul from watching this movie , how great will be my xxunk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTXI2snqJs26"
      },
      "source": [
        "vocab_sz = len(dls.vocab)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBQSfje_KDk3"
      },
      "source": [
        "xb, yb = dls.one_batch()\n",
        "model = ReformerLM(vocab_sz, 256, 2)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTwAZnlKRP0"
      },
      "source": [
        "# model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcKHa64ELBJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d8b425-51e2-471c-eacc-67e151c1df1f"
      },
      "source": [
        "with torch.no_grad():\n",
        "    out = model(xb.cpu())\n",
        "out.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 128, 7704])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZs0RF5hLnbG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "bf4faf7f-4728-4b9d-e9f9-0175ce6e92fa"
      },
      "source": [
        "learn = Learner(dls, ReformerLM(vocab_sz, 256, depth=2), metrics=[accuracy, Perplexity()], \n",
        "                loss_func=CrossEntropyLossFlat(), path=path, wd=0.1).to_native_fp16()\n",
        "learn.lr_find()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuggestedLRs(lr_min=0.0009120108559727668, lr_steep=0.0014454397605732083)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meDguK_ZQlkR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8cb3c964-c8f2-4239-ce47-43e9659ac371"
      },
      "source": [
        "learn.fit_one_cycle(5, 2e-3)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>6.438759</td>\n",
              "      <td>6.000758</td>\n",
              "      <td>0.090703</td>\n",
              "      <td>403.734772</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.544880</td>\n",
              "      <td>5.518611</td>\n",
              "      <td>0.133194</td>\n",
              "      <td>249.288635</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>5.198312</td>\n",
              "      <td>5.317602</td>\n",
              "      <td>0.148290</td>\n",
              "      <td>203.894287</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.949385</td>\n",
              "      <td>5.265206</td>\n",
              "      <td>0.153843</td>\n",
              "      <td>193.486130</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.753153</td>\n",
              "      <td>5.268610</td>\n",
              "      <td>0.154512</td>\n",
              "      <td>194.145828</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cmy4MBm08Ry"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}