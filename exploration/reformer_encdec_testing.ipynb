{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test basic text task with reformer enc_dec:  \n",
    "https://github.com/lucidrains/reformer-pytorch#reformer-encoder-decoder-architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from reformer_pytorch import ReformerEncDec\n",
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use IMDB_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.&lt;br /&gt;&lt;br /&gt;But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  \\\n",
       "0  negative   \n",
       "1  positive   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                                                                                    Un-bleeping-believable! Meg Ryan doesn't even look her usual pert lovable self in this, which normally makes me forgive her shallow ticky acting schtick. Hard to believe she was the producer on this dog. Plus Kevin Kline: what kind of suicide trip has his career been on? Whoosh... Banzai!!! Finally this was directed by the guy who did Big Chill? Must be a replay of Jonestown - hollywood style. Wooofff!   \n",
       "1  This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is som...   \n",
       "\n",
       "   is_valid  \n",
       "0     False  \n",
       "1     False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128, 1.0),\n",
       " (256, 0.983),\n",
       " (384, 0.946),\n",
       " (512, 0.907),\n",
       " (640, 0.852),\n",
       " (768, 0.693),\n",
       " (896, 0.589),\n",
       " (1024, 0.511)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mostly long texts:\n",
    "[(sl, (df['text'].apply(len)>sl).mean()) for sl in range(128,1024+1, 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> is eos correctly applied?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_eos(text):\n",
    "    return text + f' {EOS}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataBlock(blocks=TextBlock.from_df('text', is_lm=True, rules=[add_eos], seq_len=seq_len),\n",
    "                get_x=ColReader('text'), \n",
    "                splitter=RandomSplitter(valid_pct=0.1)\n",
    "               ).dataloaders(df, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" How To Lose Friends &amp; Alienate People \" is not based on xxunk Woods ' xxunk . It is a mediocre romantic comedy based on Toby Young 's book on his experiences working as a journalist covering xxunk . The film stars Simon Pegg as Sidney Young , a xxunk British journalist who takes a job in an illustrious celebrity magazine in New York . Young is xxunk in getting caught up all type of shenanigans to xxunk all around him , hence movie title . He is xxunk , daring , and moronic . But nevertheless for some very bizarre reason , he is a somewhat likable character . Sidney xxunk a fellow journalist , the composed Alison xxunk , played quite xxunk by xxunk xxunk</td>\n",
       "      <td>How To Lose Friends &amp; Alienate People \" is not based on xxunk Woods ' xxunk . It is a mediocre romantic comedy based on Toby Young 's book on his experiences working as a journalist covering xxunk . The film stars Simon Pegg as Sidney Young , a xxunk British journalist who takes a job in an illustrious celebrity magazine in New York . Young is xxunk in getting caught up all type of shenanigans to xxunk all around him , hence movie title . He is xxunk , daring , and moronic . But nevertheless for some very bizarre reason , he is a somewhat likable character . Sidney xxunk a fellow journalist , the composed Alison xxunk , played quite xxunk by xxunk xxunk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>final moments and , of course , steals the show . Dancy and Wilson are well worth the price of xxunk too.&lt;br /&gt;&lt;br /&gt;As with \" Away from Her , \" \" Evening \" stays with you at length , xxunk a re - thinking its story and characters , and re - experiencing the emotions it raises . At two hours , the film runs a bit long , but the way it stays with you xxunk is welcome among the many movies that go cold long before your popcorn . xxeos I had heard good things about this film and was , you guessed it , a bit disappointed . Reese xxunk is as promised surprisingly good , surprisingly confident , at a young age ;</td>\n",
       "      <td>moments and , of course , steals the show . Dancy and Wilson are well worth the price of xxunk too.&lt;br /&gt;&lt;br /&gt;As with \" Away from Her , \" \" Evening \" stays with you at length , xxunk a re - thinking its story and characters , and re - experiencing the emotions it raises . At two hours , the film runs a bit long , but the way it stays with you xxunk is welcome among the many movies that go cold long before your popcorn . xxeos I had heard good things about this film and was , you guessed it , a bit disappointed . Reese xxunk is as promised surprisingly good , surprisingly confident , at a young age ; really</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab. 1 is pad, 3 is eos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#7704) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the'...],\n",
       " 'xxpad',\n",
       " 'xxeos')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L(dls.vocab), PAD, EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test for eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xb, _ in dls.train:\n",
    "#     if (xb==3).sum() > 0:\n",
    "#         for b in xb:\n",
    "#             if (b==3).sum()>0:\n",
    "#                 print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the paper - synthetic task specification. Try similar for text task:\n",
    ">To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256 and 4 heads. We train it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2 and nrounds = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ReformerLM(\n",
    "#     num_tokens= 20000,\n",
    "#     dim = 1024,\n",
    "#     depth = 12,\n",
    "#     max_seq_len = 8192,\n",
    "#     heads = 8,\n",
    "#     lsh_dropout = 0.1,\n",
    "#     ff_dropout = 0.1,\n",
    "#     post_attn_dropout = 0.1,\n",
    "#     layer_dropout = 0.1,  # layer dropout from 'Reducing Transformer Depth on Demand' paper\n",
    "#     causal = True,        # auto-regressive or not\n",
    "#     bucket_size = 64,     # average size of qk per bucket, 64 was recommended in paper\n",
    "#     n_hashes = 4,         # 4 is permissible per author, 8 is the best but slower\n",
    "#     emb_dim = 128,        # embedding factorization for further memory savings\n",
    "#     dim_head = 64,        # be able to fix the dimension of each head, making it independent of the embedding dimension and the number of heads\n",
    "#     ff_chunks = 200,      # number of chunks for feedforward layer, make higher if there are memory issues\n",
    "#     attn_chunks = 8,      # process lsh attention in chunks, only way for memory to fit when scaling to 16k tokens\n",
    "#     num_mem_kv = 128,       # persistent learned memory key values, from all-attention paper\n",
    "#     twin_attention = False, # both branches of the reversible network will be attention\n",
    "#     full_attn_thres = 1024, # use full attention if context length is less than set value\n",
    "#     reverse_thres = 1024,   # turn off reversibility for 2x speed for sequence lengths shorter or equal to the designated value\n",
    "#     use_scale_norm = False,  # use scale norm from 'Transformers without tears' paper\n",
    "#     use_rezero = False,      # remove normalization and use rezero from 'ReZero is All You Need'\n",
    "#     one_value_head = False,  # use one set of values for all heads from 'One Write-Head Is All You Need'\n",
    "#     weight_tie = False,           # tie parameters of each layer for no memory per additional depth\n",
    "#     weight_tie_embedding = False, # use token embedding for projection of output, some papers report better results\n",
    "#     n_local_attn_heads = 2,       # many papers suggest mixing local attention heads aids specialization and improves on certain tasks\n",
    "#     pkm_layers = (4,7),           # specify layers to use product key memory. paper shows 1 or 2 modules near the middle of the transformer is best\n",
    "#     pkm_num_keys = 128,           # defaults to 128, but can be increased to 256 or 512 as memory allows\n",
    "#     use_full_attn = True    # only turn on this flag to override and turn on full attention for all sequence lengths. for comparison with LSH to show that it is working\n",
    "# ).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the training loop from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(dls.vocab) # number of tokens in dataloader\n",
    "n_layers = 1\n",
    "n_heads = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">how to set ff-dim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: enc_dec parameters must be specified with 'enc' or 'dec' suffix\n",
    "\n",
    "model = ReformerEncDec(\n",
    "    dim = 256,\n",
    "    \n",
    "    enc_heads = 4, ## ok??\n",
    "    enc_num_tokens = num_tokens,\n",
    "    enc_depth = n_layers,\n",
    "    enc_max_seq_len = seq_len,\n",
    "    \n",
    "    dec_num_tokens = num_tokens,\n",
    "    dec_depth = n_layers,\n",
    "    dec_heads = n_heads, ## ok??\n",
    "    dec_max_seq_len = seq_len\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422232"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Num params:\n",
    "sum([d.flatten().shape[0] for d in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a basic optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: loss function is built in to model**  \n",
    "Training loop is a bit atypical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iteration: 0, losss: 9.089465141296387\n",
      "epoch: 0, iteration: 50, losss: 8.882287455540077\n",
      "epoch: 0, iteration: 100, losss: 8.603285345700709\n",
      "epoch: 0, iteration: 150, losss: 8.36748148116055\n",
      "epoch: 0, iteration: 200, losss: 8.186546071844907\n",
      "epoch: 1, iteration: 0, losss: 7.306553840637207\n",
      "epoch: 1, iteration: 50, losss: 7.287906702827005\n",
      "epoch: 1, iteration: 100, losss: 7.218148396746947\n",
      "epoch: 1, iteration: 150, losss: 7.168114295858421\n",
      "epoch: 1, iteration: 200, losss: 7.122259317938961\n",
      "epoch: 2, iteration: 0, losss: 6.853013515472412\n",
      "epoch: 2, iteration: 50, losss: 6.8687695241441915\n",
      "epoch: 2, iteration: 100, losss: 6.852435890990908\n",
      "epoch: 2, iteration: 150, losss: 6.8119382637226025\n",
      "epoch: 2, iteration: 200, losss: 6.794969864745639\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): \n",
    "    \n",
    "    running_loss = []\n",
    "    \n",
    "    for i, (xb, yb) in enumerate(dls.train):\n",
    "        \n",
    "        #the model crashes on sequences other than 128\n",
    "        if xb.shape[1] != 128:\n",
    "            continue\n",
    "            \n",
    "        # need to pass a mask, check for token id 1 - the pad token in fastai\n",
    "        input_mask = xb==1\n",
    "                \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # this is how the loss is calculated in the documentation - forward and loss in combined in model's forward pass\n",
    "        loss = model(xb, yb, return_loss = True, enc_input_mask = input_mask) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss.append(loss.item())\n",
    "        if i%50 == 0:\n",
    "            print(f'epoch: {epoch}, iteration: {i}, losss: {np.mean(running_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
