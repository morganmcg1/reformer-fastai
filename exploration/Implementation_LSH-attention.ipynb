{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from einops import rearrange, repeat, reduce\n",
    "from functools import wraps\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simplified implementation of LSH-attention based on [lucidrains](https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py).\n",
    "\n",
    "\n",
    "**Not implemented:**\n",
    "* Direct attention masking. This means passing in a [sl x sl] mask for the attention matrix (**Not** the same as casual masking)\n",
    "* Variable query length (query length is always sequence length)\n",
    "* Attention across buckets (we assume attention only within bucket - as suggested in paper)\n",
    "\n",
    "**Note:**\n",
    "* For a detailed walkthru of the code, see this exploration [notebook](https://github.com/morganmcg1/reformer-fastai/blob/main/exploration/LSH-attention.ipynb)\n",
    "* 8 hash rounds with 64 buckets seems is suggested in paper/trax\n",
    "* We'll assume rehashing in each hash round as in the trax library. Otherwise we have to reuse our hashes in some way.\n",
    "* We'll assume the same random rotations for all items in a batch (i.e. copy rotations across batch dimension). Lucidrains' source code refers to this as `random_rotations_per_head` which can confusing imo. In this Module it refers to if we copy or draw new random rotations across the batch dimension, i.e. for each item. However in the **LSHSelfAttention** module we reshape our input to include the attention head dimension. This means that each head has the same rotations (since our items only have 1 set of rotations). *We can consider adding this extra functionality later.*\n",
    "* Normalize **key vectors** (but not q vectors)\n",
    "* Attention is optionally returned, but unsorted. Need to take care of unsorting before use.\n",
    "* Duplicate attention defaults to False. That means that k/q pairs that end up in the same attention chunk across hash rounds get's penalized with the log of the count.\n",
    "* Summation of duplicate attention is done with chunked summations for memory concerns. But same result to normal summation.\n",
    "* Mask value is `TOKEN_SELF_ATTN_VALUE = -5e4`, lucidrains notes that \"carefully set for half precision to work\"\n",
    "\n",
    "**Masking**:\n",
    "* Other hash buckets (LSH-specific)\n",
    "* mask self/self pairs (LSH-specific), i.e. k and q at position i can't attend to each other\n",
    "* pad mask\n",
    "* casual masking\n",
    "\n",
    "**Code**:\n",
    "* Uses fastai Module - no super().__init__()\n",
    "* uses fastcore store_attr() to store self.attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "# boundaries might occur in the middle of a sequence of items from the\n",
    "# same bucket, so this increases the chances of attending to relevant items.\n",
    "def look_one_back(x):\n",
    "    x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n",
    "    return torch.cat([x, x_extra], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key_val(t1, t2, dim=-1):\n",
    "    values, indices = t1.sort(dim=dim)\n",
    "    t2 = t2.expand_as(t1)\n",
    "    return values, t2.gather(dim, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, default_val):\n",
    "    return default_val if val is None else val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dims(ind_from, ind_to, tensor):\n",
    "    shape = list(tensor.shape)\n",
    "    arr_slice = slice(ind_from, ind_to + 1)\n",
    "    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n",
    "    return tensor.reshape(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_sum(tensor, chunks=1):\n",
    "    *orig_size, last_dim = tensor.shape\n",
    "    tensor = tensor.reshape(-1, last_dim)\n",
    "    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n",
    "    return torch.cat(summed_tensors, dim=0).reshape(orig_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n",
    "            namespace_str = str(default(key_namespace, ''))\n",
    "            _cache = getattr(self, cache_attr)\n",
    "            _keyname = f'{cache_namespace}:{namespace_str}'\n",
    "\n",
    "            if fetch:\n",
    "                val = _cache[_keyname]\n",
    "                if reexecute:\n",
    "                    fn(self, *args, **kwargs)\n",
    "            else:\n",
    "                val = fn(self, *args, **kwargs)\n",
    "                if set_cache:\n",
    "                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n",
    "            return val\n",
    "        return wrapper\n",
    "    return inner_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inputs_chunk(fn, chunks=1, dim=0):\n",
    "    def inner_fn(*args, **kwargs):\n",
    "        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n",
    "        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n",
    "        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n",
    "        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n",
    "        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n",
    "    return inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSHAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHAttention(Module):\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,                       # attention matrix dropout\n",
    "                  bucket_size = 64,                   # at least 64 suggested in trax\n",
    "                  n_hashes = 8,                       # papers sugests 8\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = False,  # as in the paper\n",
    "                  attend_across_buckets = False,      # as in the paper\n",
    "                  drop_for_hash_rate = 0.0,           # unsure of default, not mentioned in paper\n",
    "                  return_attn = False):\n",
    "        \n",
    "        if dropout >= 1.0 or drop_for_hash_rate >=1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "        \n",
    "        store_attr(but=['dropout', 'drop_for_hash_rate'])  # fastcore - store attibutes\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
    "        self._cache = {} # cache buckets for reversible network, required to make Reformer work at depth\n",
    "\n",
    "    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        # 0. We need an even number of buckets: \n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        # 1. account for the input shapes. vecs = [bs, sl, dim]\n",
    "        batch_size, seqlen, dim = vecs.shape\n",
    "        device = vecs.device\n",
    "        rotations_shape = (dim, self.n_hashes, n_buckets // 2)\n",
    "\n",
    "        # 2. Calculate hash bucket id via random rotations, concatenation and argmax \n",
    "        # note: we copy rotations accross batch dimension (see exploration notebook for details). \n",
    "        random_rotations = repeat(torch.randn(rotations_shape), \n",
    "                                  'd nh nb -> bs d nh nb', bs=batch_size)           \n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "                       \n",
    "        rotated_vecs = torch.einsum('bsd,bdhn->bhsn', \n",
    "                                    dropped_vecs,       # [bs, sl, dim]\n",
    "                                    random_rotations)   # [bs, dim, n_hashes, n_buckets//2]\n",
    "                                                        # rotated vecs: [bs, n_hashes, sl, n_buckets//2]\n",
    "\n",
    "        rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1) # [bs, n_hashes, sl, n_buckets]\n",
    "        buckets = torch.argmax(rotated_vecs, dim=-1)                    # [bs, n_hashes, sl] \n",
    "\n",
    "        # 3. Next we add offsets so that bucket numbers from different hashing rounds don't overlap.\n",
    "        # We also reshape the buckets so that each hash round is concatenated along the -1 dim\n",
    "        offsets = torch.arange(self.n_hashes)                              # list of [0,1,2,..n_hashes-1]\n",
    "        offsets = rearrange(offsets * n_buckets, 'nh -> 1 nh 1')        # [1, n_hashes, 1]\n",
    "        buckets = rearrange(buckets+offsets, 'bs nh sl -> bs (nh sl)')  # [bs, (n_hashes*sl)]\n",
    "        return buckets\n",
    "\n",
    "    def forward(self, qk, v, input_mask = None, **kwargs):\n",
    "        batch_size, seqlen, dim, device = *qk.shape, qk.device\n",
    "\n",
    "        # caching\n",
    "        is_reverse = kwargs.pop('_reverse', False)\n",
    "        depth = kwargs.pop('_depth', None)\n",
    "        \n",
    "        # We will have an even number of buckets, and our attention chunks needs to fit completely within a seqlen\n",
    "        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n",
    "        \n",
    "        # get the hash buckets for our qk input vectors\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n",
    "\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "        \n",
    "        # Create an index that reflexts both bucket id and sequence id. This let's us sort qk according \n",
    "        # to both simultaneously. Repeated across the batch dimension.\n",
    "        ticker = repeat(torch.arange(self.n_hashes * seqlen), 'l -> bs l', bs=batch_size)\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen) \n",
    "        buckets_and_t = buckets_and_t.detach()                # [bs, seqlen*n_hashes]\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)  # [bs, seqlen*n_hashes]\n",
    "        _, undo_sort = sticker.sort(dim=-1)                                    # indexes to undo sortings\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = sbuckets_and_t.detach()   # no need to store gradiens for indexes\n",
    "        sticker = sticker.detach()\n",
    "        undo_sort = undo_sort.detach()\n",
    "\n",
    "        st = (sticker % seqlen)             # index of [0..seqlen-1] for each hash round\n",
    "        sqk = batched_index_select(qk, st)  # get the sorted qk, [bs, seqlen*n_hashes, dim]\n",
    "        sv = batched_index_select(v, st)    # get the sorted v, [bs, seqlen*n_hashes, dim] \n",
    "\n",
    "        # Reshape to include a n_chunks axis.\n",
    "        n_chunks = self.n_hashes * n_buckets\n",
    "        bq_t = bkv_t = rearrange(st, 'bs (n s) -> bs n s', n=n_chunks) # [bs, n_chunks, chunk_size]\n",
    "        bqk = rearrange(sqk, 'bs (n s) d -> bs n s d', n=n_chunks)     # [bs, n_chunks, chunk_size, dim]\n",
    "        bv = rearrange(sv, 'bs (n s) d -> bs n s d', n=n_chunks)       # [bs, n_chunks, chunk_size, dim]\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bq = bqk\n",
    "        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        # Note: no look_back for queries\n",
    "\n",
    "        bk = look_one_back(bk)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bv = look_one_back(bv)        # [bs, n_chunks, chunk_size*2, dim]\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "\n",
    "        # Dot-product attention.\n",
    "        dots = torch.einsum('bnsd,bnzd->bnsz', \n",
    "                    bq,                  # [bs, n_chunks, chunk_size, dim]\n",
    "                    bk                   # [bs, n_chunks, chunk_size*2, dim]\n",
    "                   ) * (dim ** -0.5)     # dots: [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "        masked_value = max_neg_value(dots)\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n",
    "            mq = input_mask.gather(1, st).reshape((batch_size, n_chunks, -1))\n",
    "            mkv = look_one_back(mq)\n",
    "            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
    "            dots.masked_fill_(mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self.attend_across_buckets:\n",
    "            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, n_chunks, -1))\n",
    "            bkv_buckets = look_one_back(bkv_buckets)\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots.masked_fill_(bucket_mask, masked_value)\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition.\n",
    "        \n",
    "        if not self.allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % n_chunks\n",
    "            if not self.attend_across_buckets:\n",
    "                locs1 = buckets * n_chunks + locs1\n",
    "                locs2 = buckets * n_chunks + locs2\n",
    "            locs = torch.cat([\n",
    "                torch.reshape(locs1, (batch_size, self.n_hashes, seqlen)),\n",
    "                torch.reshape(locs2, (batch_size, self.n_hashes, seqlen)),\n",
    "            ], 1).permute((0, 2, 1))\n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = torch.reshape(slocs, (batch_size, n_chunks, -1, 2 * self.n_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :self.n_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, self.n_hashes))\n",
    "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(self.n_hashes * batch_size))\n",
    "            dup_counts = dup_counts.detach()\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - torch.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
    "        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n",
    "        dropped_dots = self.dropout(dots)\n",
    "        \n",
    "        # calculate self-attention (attn * values)\n",
    "        bo = torch.einsum('bnsz,bnzd->bnsd', \n",
    "                          dropped_dots,      # [bs, n_chunks, chunk_size, chunk_size*2]\n",
    "                          bv)                # [bs, n_chunks, chunk_size*2, dim]    \n",
    "                                             # bo: [bs, n_chunks, chunk_size, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape self-attention\n",
    "        so = rearrange(bo, 'b n s d -> b (n s) d')                     # [bs, seqlen*n_hashes, dim]\n",
    "        o = batched_index_select(so, undo_sort)                        # [bs, seqlen*n_hashes, dim]\n",
    "        o = rearrange(o, 'b (nh sl) d -> b nh sl d', nh=self.n_hashes) # [bs, n_hashes, seqlen, dim]\n",
    "        \n",
    "        # unchunk, unsort and reshape logits\n",
    "        slogits = rearrange(dots_logsumexp, 'bs n s 1 -> bs (n s)')              # [bs, seqlen*n_hashes]\n",
    "        logits = slogits.gather(1, undo_sort)                                    # [bs, seqlen*n_hashes]\n",
    "        logits = rearrange(logits, 'bs (nr sl) -> bs nr sl 1', nr=self.n_hashes) # [bs, n_hashes, seqlen, 1]\n",
    "        \n",
    "        # average probabilites across hash rounds (dim 1) and get weighted attention\n",
    "        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True)) # [bs, n_rounds, seqlen, 1]\n",
    "        out = torch.sum(o * probs, dim=1)                                        # [bs, seqlen, dim]\n",
    "\n",
    "        # return unsorted attention weights - empty otherwise\n",
    "        attn = torch.empty(0, device=device)\n",
    "        if self.return_attn:\n",
    "            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            attn_unsort = attn_unsort.view(batch_size * self.n_hashes, -1).long()\n",
    "            unsorted_dots = torch.zeros(batch_size * self.n_hashes, seqlen * seqlen, device=device)\n",
    "            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n",
    "            del attn_unsort\n",
    "            unsorted_dots = unsorted_dots.reshape(batch_size, self.n_hashes, seqlen, seqlen)\n",
    "            attn = torch.sum(unsorted_dots * probs, dim=1)\n",
    "\n",
    "        # return output, attention matrix, and bucket distribution\n",
    "        return out, attn, buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 512, 128]), torch.Size([64, 4096]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qk = torch.randn(64, 512, 128)\n",
    "v = torch.rand(64, 512, 128)\n",
    "lsh_att = LSHAttention()\n",
    "out, attn, buckets = lsh_att(qk, v)\n",
    "out.shape, buckets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSHSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation is based on [lucidrains](https://github.com/lucidrains/reformer-pytorch/blob/master/reformer_pytorch/reformer_pytorch.py), but simplified and refactored.\n",
    "\n",
    "**Note:**\n",
    "* Only supports LSH attention, **not** full attention\n",
    "* Direct attention masking (`input_attn_mask`) is not implemented\n",
    "* [Local Attention](https://github.com/lucidrains/local-attention) feature has been removed - not part of reformer paper.\n",
    "* Post attention dropout is a final dropout directly before output. Not standard I believe, but the feature is lef in. Default rate is 0.\n",
    "* `one_value_per_head` is removed. We assume that each head will have it's own values - this is standard for the transformer.\n",
    "* `random_rotations_per_head` is assumed to be False, and is removed from base `LSHAttention` layer. Consider adding it back in (not much work).\n",
    "* `mem_kv` is removed. Not sure what this does, and not found in standard transformer to my knowledge.\n",
    "* Arguments passed on to LSHAttention have similar defaults (and reasoning)\n",
    "\n",
    "**A note on `dim`:**\n",
    "* `dim` can be a bit of an overloaded term in transformers.\n",
    "* LSHSelfAttentin has a model_`dim`. We require that our input tokens are embedded to fit this dimension. So `emb_dim` of our input and `model_dim` are similar.\n",
    "* LSHSelfAttention will have several heads. Each head has a `dim_head`. The defalt setting is to divide `model_dim` equally over each head. In this case we require `model_dim % n_head = 0`. We can also set `dim_head` manually. In this case the dim of all the heads `n_heads * dim_head` will be different from model dim. We use the final feed forward layer to convert the output back into model `dim`.\n",
    "* LSHAttention also has a `dim` in its `forward()`. This is the dim of `qk` and `v` that it was called with. In the setting of LSHSelfAttention, LSHAttention `dim` will be the same as `dim_head`.\n",
    "\n",
    "**Todo:**\n",
    "* add feature to return attention matrix (calculated, but not returned at the moment)\n",
    "* consider reintroducing `random_rotation_per_head` - in `LSHAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHSelfAttention(Module):\n",
    "    def __init__(self, \n",
    "                 dim,                                 # Note: dim refers to model dim/similar to embedding dim for input\n",
    "                 n_heads = 8, \n",
    "                 bucket_size = 64,                    # reccomended default from paper/lucid\n",
    "                 n_hashes = 8,                        # reccomended default from paper/lucid\n",
    "                 causal = False, \n",
    "                 dim_head = None, \n",
    "                 attend_across_buckets = False,      \n",
    "                 allow_duplicate_attention = False,   # Penalize multiple qk-v pairs in same attention chunk or not\n",
    "                 return_attn = False,                 # Not implemented yet\n",
    "                 dropout = 0., \n",
    "                 post_attn_dropout = 0.,              # a final dropout on output (not standard)\n",
    "                 **kwargs):\n",
    "         \n",
    "        assert dim_head or (dim % n_heads) == 0, 'dimensions must be divisible by number of heads'\n",
    "        \n",
    "        dim_head = default(dim_head, dim // n_heads)  # dim single head\n",
    "        dim_heads = dim_head * n_heads                # dim all heads\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.toqk = nn.Linear(dim, dim_heads, bias = False)\n",
    "        self.tov = nn.Linear(dim, dim_heads, bias = False)\n",
    "        self.to_out = nn.Linear(dim_heads, dim)\n",
    "        \n",
    "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, \n",
    "                                     attend_across_buckets = attend_across_buckets,  \n",
    "                                     allow_duplicate_attention = allow_duplicate_attention, \n",
    "                                     return_attn = return_attn, dropout = dropout, **kwargs)\n",
    "        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n",
    "\n",
    "    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, **kwargs):\n",
    "        device, dtype = x.device, x.dtype\n",
    "        bs, sl, emb_dim = x.shape\n",
    "\n",
    "        keys = default(keys, torch.empty(bs, 0, emb_dim, dtype=dtype, device=device))\n",
    "        c = keys.shape[1]\n",
    "\n",
    "        # project qk and v\n",
    "        x = torch.cat((x, keys), dim=1)  # [bs, sl+keys.shape[1], dim]\n",
    "        qk = self.toqk(x)                # [bs, sl, dim_heads (dim_head * heads)]\n",
    "        v = self.tov(x)                  # [bs, sl, dim_heads]\n",
    "        \n",
    "        # split off head dimension for qk and v. Resulting shapes are: [nh, bs, sl, dim_head]\n",
    "        qk, v = map(lambda t: rearrange(t, 'bs sl (nh dh) -> nh bs sl dh', nh=self.n_heads), (qk, v))\n",
    "        \n",
    "        # masks have shape [bs, sl] and are maybe concatenated [bs, sl*2]\n",
    "        mask = None\n",
    "        if input_mask is not None or context_mask is not None:\n",
    "            default_mask = torch.tensor([True], device=device)\n",
    "            i_mask = default(input_mask, default_mask.expand(bs, sl))\n",
    "            c_mask = default(context_mask, default_mask.expand(bs, c))\n",
    "            mask = torch.cat((i_mask, c_mask), dim=1)\n",
    "        \n",
    "        # run lsh per head (iterate through 0th dim i.e. the n_head dim), concatenate and rearrange\n",
    "        # Note: masks are reused per head\n",
    "        lsh_results = L([self.lsh_attn(qk_h, v_h, mask) for qk_h, v_h in zip(qk, v)])  \n",
    "        out = lsh_results.itemgot(0)                                   # split tuple (output, attn, buckets)\n",
    "        out = torch.cat([head for head in out], dim=0)                 # concatenate [n_heads*bs, sl, dh]\n",
    "        out = rearrange(out, '(nh bs) sl dh -> bs sl (nh dh)', bs=bs)  # [bs, sl, dim_heads] (dim_heads = head_dim * n_heads)\n",
    "        \n",
    "        # pass through final feed forward and maybe dropout\n",
    "        out = self.to_out(out)                                            # [bs, sl, dim]\n",
    "        return self.post_attn_dropout(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1024, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = LSHSelfAttention(\n",
    "    dim = 128,\n",
    "    n_heads = 8,\n",
    "    bucket_size = 64,\n",
    "    n_hashes = 8,\n",
    "    dim_head = 64,\n",
    "    causal = False\n",
    ")\n",
    "x = torch.randn(10, 1024, 128)\n",
    "i_mask = torch.ones(10, 1024).bool() # bs x sl\n",
    "c_mask = torch.ones(10, 1024).bool() # bs x sl\n",
    "attn(x, input_mask=i_mask, contex_mask=c_mask).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "365.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
