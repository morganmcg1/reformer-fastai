{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "Copy of LM_exp_template.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganmcg1/reformer-fastai/blob/main/experiments/LM_exp_template_wandb_AdaFactor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCQkEFpsrHkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0132c040-4b2b-4ea6-d2a0-0bfb903235b8"
      },
      "source": [
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "  !pip install -Uqq fastai einops datasets wandb yappi gprof2dot pyinstrument"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 194kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 21.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 27.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 8.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.7MB 339kB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 55.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 51.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 50.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25h  Building wheel for yappi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gprof2dot (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nZDOaDK8_m3"
      },
      "source": [
        "Note: Restart the runtime after installing fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ylWnbN8zlN"
      },
      "source": [
        "## Download the tranformer and tokenizer code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gy8rSRxvYg4"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/morganmcg1/reformer-fastai/main/basic_tokenizers.py\n",
        "!wget -q https://raw.githubusercontent.com/morganmcg1/reformer-fastai/main/basic_transformer.py"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wpug9knrHkY"
      },
      "source": [
        "## Download and Unpack enwik8 Data\n",
        "\n",
        "Download and unzip enwik8 data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JePpPJyMrHkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe66867-bcdb-4efc-a1be-94781968116b"
      },
      "source": [
        "!wget -P data/ http://mattmahoney.net/dc/enwik8.zip\n",
        "!unzip data/enwik8.zip -d data/\n",
        "!ls data\n",
        "!head -n 132 data/enwik8"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-27 20:20:42--  http://mattmahoney.net/dc/enwik8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.24\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.24|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36445475 (35M) [application/zip]\n",
            "Saving to: ‘data/enwik8.zip’\n",
            "\n",
            "enwik8.zip          100%[===================>]  34.76M   703KB/s    in 51s     \n",
            "\n",
            "2020-11-27 20:21:33 (694 KB/s) - ‘data/enwik8.zip’ saved [36445475/36445475]\n",
            "\n",
            "Archive:  data/enwik8.zip\n",
            "  inflating: data/enwik8             \n",
            "enwik8\tenwik8.zip\n",
            "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\n",
            "  <siteinfo>\n",
            "    <sitename>Wikipedia</sitename>\n",
            "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n",
            "    <generator>MediaWiki 1.6alpha</generator>\n",
            "    <case>first-letter</case>\n",
            "      <namespaces>\n",
            "      <namespace key=\"-2\">Media</namespace>\n",
            "      <namespace key=\"-1\">Special</namespace>\n",
            "      <namespace key=\"0\" />\n",
            "      <namespace key=\"1\">Talk</namespace>\n",
            "      <namespace key=\"2\">User</namespace>\n",
            "      <namespace key=\"3\">User talk</namespace>\n",
            "      <namespace key=\"4\">Wikipedia</namespace>\n",
            "      <namespace key=\"5\">Wikipedia talk</namespace>\n",
            "      <namespace key=\"6\">Image</namespace>\n",
            "      <namespace key=\"7\">Image talk</namespace>\n",
            "      <namespace key=\"8\">MediaWiki</namespace>\n",
            "      <namespace key=\"9\">MediaWiki talk</namespace>\n",
            "      <namespace key=\"10\">Template</namespace>\n",
            "      <namespace key=\"11\">Template talk</namespace>\n",
            "      <namespace key=\"12\">Help</namespace>\n",
            "      <namespace key=\"13\">Help talk</namespace>\n",
            "      <namespace key=\"14\">Category</namespace>\n",
            "      <namespace key=\"15\">Category talk</namespace>\n",
            "      <namespace key=\"100\">Portal</namespace>\n",
            "      <namespace key=\"101\">Portal talk</namespace>\n",
            "    </namespaces>\n",
            "  </siteinfo>\n",
            "  <page>\n",
            "    <title>AaA</title>\n",
            "    <id>1</id>\n",
            "    <revision>\n",
            "      <id>32899315</id>\n",
            "      <timestamp>2005-12-27T18:46:47Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Jsmethers</username>\n",
            "        <id>614213</id>\n",
            "      </contributor>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AlgeriA</title>\n",
            "    <id>5</id>\n",
            "    <revision>\n",
            "      <id>18063769</id>\n",
            "      <timestamp>2005-07-03T11:13:13Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Docu</username>\n",
            "        <id>8029</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>adding cur_id=5: {{R from CamelCase}}</comment>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[Algeria]]{{R from CamelCase}}</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AmericanSamoa</title>\n",
            "    <id>6</id>\n",
            "    <revision>\n",
            "      <id>18063795</id>\n",
            "      <timestamp>2005-07-03T11:14:17Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Docu</username>\n",
            "        <id>8029</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>adding to cur_id=6  {{R from CamelCase}}</comment>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[American Samoa]]{{R from CamelCase}}</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AppliedEthics</title>\n",
            "    <id>8</id>\n",
            "    <revision>\n",
            "      <id>15898943</id>\n",
            "      <timestamp>2002-02-25T15:43:11Z</timestamp>\n",
            "      <contributor>\n",
            "        <ip>Conversion script</ip>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>Automated conversion</comment>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[Applied ethics]]\n",
            "</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AccessibleComputing</title>\n",
            "    <id>10</id>\n",
            "    <revision>\n",
            "      <id>15898945</id>\n",
            "      <timestamp>2003-04-25T22:18:38Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Ams80</username>\n",
            "        <id>7543</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>Fixing redirect</comment>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[Accessible_computing]]</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AdA</title>\n",
            "    <id>11</id>\n",
            "    <revision>\n",
            "      <id>15898946</id>\n",
            "      <timestamp>2002-09-22T16:02:58Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Andre Engels</username>\n",
            "        <id>300</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[Ada programming language]]</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>Anarchism</title>\n",
            "    <id>12</id>\n",
            "    <revision>\n",
            "      <id>42136831</id>\n",
            "      <timestamp>2006-03-04T01:41:25Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>CJames745</username>\n",
            "        <id>832382</id>\n",
            "      </contributor>\n",
            "      <minor />\n",
            "      <comment>/* Anarchist Communism */  too many brackets</comment>\n",
            "      <text xml:space=\"preserve\">{{Anarchism}}\n",
            "'''Anarchism''' originated as a term of abuse first used against early [[working class]] [[radical]]s including the [[Diggers]] of the [[English Revolution]] and the [[sans-culotte|''sans-culottes'']] of the [[French Revolution]].[http://uk.encarta.msn.com/encyclopedia_761568770/Anarchism.html] Whilst the term is still used in a pejorative way to describe ''&quot;any act that used violent means to destroy the organization of society&quot;''&lt;ref&gt;[http://www.cas.sc.edu/socy/faculty/deflem/zhistorintpolency.html History of International Police Cooperation], from the final protocols of the &quot;International Conference of Rome for the Social Defense Against Anarchists&quot;, 1898&lt;/ref&gt;, it has also been taken up as a positive label by self-defined anarchists.\n",
            "\n",
            "The word '''anarchism''' is [[etymology|derived from]] the [[Greek language|Greek]] ''[[Wiktionary:&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;|&amp;#945;&amp;#957;&amp;#945;&amp;#961;&amp;#967;&amp;#943;&amp;#945;]]'' (&quot;without [[archon]]s (ruler, chief, king)&quot;). Anarchism as a [[political philosophy]], is the belief that ''rulers'' are unnecessary and should be abolished, although there are differing interpretations of what this means. Anarchism also refers to related [[social movement]]s) that advocate the elimination of authoritarian institutions, particularly the [[state]].&lt;ref&gt;[http://en.wikiquote.org/wiki/Definitions_of_anarchism Definitions of anarchism] on Wikiquote, accessed 2006&lt;/ref&gt; The word &quot;[[anarchy]],&quot; as most anarchists use it, does not imply [[chaos]], [[nihilism]], or [[anomie]], but rather a harmonious [[anti-authoritarian]] society. In place of what are regarded as authoritarian political structures and coercive economic institutions, anarchists advocate social relations based upon [[voluntary association]] of autonomous individuals, [[mutual aid]], and [[self-governance]]. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8sEJ06U85Gv"
      },
      "source": [
        "## Actual start of the experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmc4iYY0rHkX"
      },
      "source": [
        "import sys\n",
        "import six\n",
        "from fastai.text.all import *"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCTZR7olrHkX",
        "outputId": "cb784645-3682-4c84-a60b-384d0da7dbc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from basic_tokenizers import ByteTextTokenizer\n",
        "from basic_transformer import TransformerLM"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No module named 'axial_positional_embedding'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mbmHvErHkX"
      },
      "source": [
        "## Experiment Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dExn4xYHrHkX"
      },
      "source": [
        "Make sure you have wandb and are logged in:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_dhEnW8xE33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7913ab59-79f4-4196-f32a-482e38e2978b"
      },
      "source": [
        "!wandb login <your_identifier>"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsjmw1onrHkY"
      },
      "source": [
        "Load Experiment Tracking with Weights & Biases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQW9rKmwrHkY"
      },
      "source": [
        "## Wandb experiment logging\n",
        "Suggeted [wandb.init logging](https://docs.wandb.com/library/init) to help keep track of experiments:\n",
        "\n",
        "**WANDB_NAME**\n",
        "\n",
        "A specific name for a particular experiment, e.g. \"lsh_2_hash_enwik8\"\n",
        "\n",
        "**GROUP**\n",
        "\n",
        "Group identifiers will help organise and **group experiments together** in the wandb interface. Suggested identifier to use are:\n",
        "\n",
        "- \"TEST\" : for general testing\n",
        "- \"SHARED-QK\" : for Shared Query-Key experiments\n",
        "- \"LSH\" : LSH-related experiemnts\n",
        "- \"REVERSIBLE\" : reversible layers experiments\n",
        "- \"WMT\" : for the WMT task\n",
        "\n",
        "**NOTES**\n",
        "\n",
        "A longer description of the run, like a -m commit message in git. This helps you remember what you were doing when you ran this run.\n",
        "\n",
        "**CONFIG**\n",
        "\n",
        "A dictionary-like object for saving inputs to your job, like hyperparameters for a model or settings for a data preprocessing job. The config will show up in a table in the UI that you can use to group, filter, and sort runs. Keys should not have . in the names, and values should be under 10 MB.\n",
        "\n",
        "**TAGS**\n",
        "\n",
        "A list of strings, which will populate the list of tags on this run in the UI. Tags are useful for organizing runs together, or applying temporary labels like \"baseline\" or \"production\". It's easy to add and remove tags in the UI, or filter down to just runs with a specific tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAf9o5W9rHkY"
      },
      "source": [
        "import wandb\n",
        "from fastai.callback.wandb import *\n",
        "\n",
        "WANDB_NAME = 'enc_lm_enwik8'\n",
        "GROUP = 'TEST'     # Group to add a run to, e.g. \"LSH\" for LSH experiments, \"REVERSIBLE\" for reversible layers\n",
        "NOTES = 'Testing the encoder LM model works'\n",
        "CONFIG = {}\n",
        "TAGS =['enc_lm','test']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BSh2BpKrHkY"
      },
      "source": [
        "Initialise wandb logging, pleaes **do not change** `project` or `entity` (that that everything gets logged to the same place)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y22nU3TorHkZ"
      },
      "source": [
        "def read_lines(path):\n",
        "    \"\"\"Tokenizes a text file.\"\"\"\n",
        "    assert os.path.exists(path)\n",
        "    lines=[]\n",
        "    with open(path, 'r') as f:\n",
        "        tokens = 0\n",
        "        for line in f:\n",
        "            lines.append(line)  # + ['<eos>'])            \n",
        "    return lines"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSzL8fXo9Ol6"
      },
      "source": [
        "## Load the file into the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5JGuwLSrHkZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "c9bf505d-4ca8-41a8-b5d5-4357b7b8c489"
      },
      "source": [
        "enwik8 = read_lines('data/enwik8')\n",
        "df = pd.DataFrame({'text':enwik8})\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1128024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\"&gt;\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;siteinfo&gt;\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;sitename&gt;Wikipedia&lt;/sitename&gt;\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;base&gt;http://en.wikipedia.org/wiki/Main_Page&lt;/base&gt;\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;generator&gt;MediaWiki 1.6alpha&lt;/generator&gt;\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                         text\n",
              "0  <mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\\n\n",
              "1                                                                                                                                                                                                                                                <siteinfo>\\n\n",
              "2                                                                                                                                                                                                                            <sitename>Wikipedia</sitename>\\n\n",
              "3                                                                                                                                                                                                       <base>http://en.wikipedia.org/wiki/Main_Page</base>\\n\n",
              "4                                                                                                                                                                                                                 <generator>MediaWiki 1.6alpha</generator>\\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lIWg-9LrHkb"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSHZhouKrHkb"
      },
      "source": [
        "Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgNCOyHdrHkb"
      },
      "source": [
        "bte = ByteTextTokenizer(is_lm=True, add_bos=True, add_eos=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgAo72a-rHkb"
      },
      "source": [
        "# # Patch the LMDataloader\n",
        "# @patch\n",
        "# def create_item(self:LMDataLoader, seq):\n",
        "#     if seq>=self.n: raise IndexError\n",
        "#     sl = self.last_len if seq//self.bs==self.n_batches-1 else self.seq_len\n",
        "#     st = (seq%self.bs)*self.bl + (seq//self.bs)*self.seq_len\n",
        "#     txt = self.chunks[st : st+sl+1]    \n",
        "#     return LMTensorText(txt[:-1]),txt[1:]\n",
        "#     # return LMTensorText(txt[:-1].tolist()+[1]), LMTensorText(txt[1:].tolist()+[1])   ## ADD EOS TOKEN"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgWyWSwsrHkb"
      },
      "source": [
        "Calc splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8B9dre4Jop"
      },
      "source": [
        "# Back up the original dataframe\n",
        "df_bckp=df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlURQqzk4Lvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9396bff-62f4-407d-f17d-c4904b09faa4"
      },
      "source": [
        "# Take 4000 records for sake of performance\n",
        "df=df_bckp[:4000]\n",
        "df.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4000 entries, 0 to 3999\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text    4000 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 31.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO6t0o7d4bSI"
      },
      "source": [
        "# Split enwik8 by character count\n",
        "a=df.apply(lambda x: x.str.len())\n",
        "a.columns=['lens']\n",
        "df=pd.concat([df,a], axis=1)\n",
        "df['lens_cum_sum'] = df.lens.cumsum()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyPjLi846DfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651e570e-5f50-4ccc-85cf-b4af6a702276"
      },
      "source": [
        "df.lens.sum()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "347377"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpqfuPk4rHkb"
      },
      "source": [
        "train_cutoff = int(df.lens.sum()*0.5) #- 10000000  # keep all but 10M characters for val and test\n",
        "    \n",
        "train_idxs = df.loc[df['lens_cum_sum'] < train_cutoff].index.values\n",
        "train_idxs = list(range(0, max(train_idxs)))\n",
        "\n",
        "remaining_idxs = len(df) - max(train_idxs)\n",
        "validation_idxs = list(range(max(train_idxs), max(train_idxs) + int(remaining_idxs/2)))\n",
        "test_idxs = list(range(max(validation_idxs), len(df)))\n",
        "\n",
        "splits = [train_idxs, validation_idxs]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skA7C-v8Z0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4348e1f-d233-497e-d78c-2b97c8cc2fc0"
      },
      "source": [
        "len(train_idxs),len(validation_idxs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2253, 874)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzsM7S4hrHkb"
      },
      "source": [
        "Get dls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN9CBITSrHkb"
      },
      "source": [
        "# Quick naive split alternative\n",
        "# cut = int(len(df)*0.8)\n",
        "# splits = range_of(df)[:cut], range_of(df[cut:])\n",
        "\n",
        "tfms = [attrgetter(\"text\"), bte]\n",
        "dsets = Datasets(df, [tfms, tfms], splits=splits, dl_type=LMDataLoader)\n",
        "\n",
        "vocab_sz = bte.vocab_size\n",
        "bs,sl = 32,128\n",
        "pad_seq2seq = partial(pad_input, pad_idx=bte.pad_token_id, pad_fields=[0,1])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjnVuZWQjzpR"
      },
      "source": [
        "## Yappi - Profiler 1\n",
        "#import yappi\n",
        "#yappi.start()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtcpiwrUsbW2"
      },
      "source": [
        "# Profiler 2\n",
        "#from pyinstrument import Profiler\n",
        "\n",
        "#profiler = Profiler()\n",
        "#profiler.start()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bIn3NLq2XMU"
      },
      "source": [
        "dls = dsets.dataloaders(bs=bs, seq_len=sl, before_batch=pad_seq2seq)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYGCDf3Usegt"
      },
      "source": [
        "# Profiler 2\n",
        "#profiler.stop()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TpGDtHUvCnQ"
      },
      "source": [
        "# Profiler 2\n",
        "#print(profiler.output_text(unicode=True, color=True))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpPsjVm3j2Qb"
      },
      "source": [
        "## Yappi - Profiler 1\n",
        "#func_stats = yappi.get_func_stats()\n",
        "#func_stats.save('callgrind.out', 'CALLGRIND')\n",
        "#yappi.stop()\n",
        "#yappi.clear_stats()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s1VRcNLlwc_"
      },
      "source": [
        "## Yappi - Profiler 1\n",
        "#!gprof2dot -f callgrind -n10 -s callgrind.out > valgrind.dot\n",
        "#!dot -Tpng valgrind.dot -o valgrind.png"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQnHRk4cDjw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "f4263e4e-29b6-46f4-a316-68ab4bb7dab9"
      },
      "source": [
        "dls.show_batch(max_n=2)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;bos&gt;      &lt;/contributor&gt;\\n&lt;eos&gt;&lt;bos&gt;&amp;lt;!-- (Please take care in adding to this list that it not grow excessively large, consider adding to</td>\n",
              "      <td>&lt;/contributor&gt;\\n&lt;eos&gt;&lt;bos&gt;&amp;lt;!-- (Please take care in adding to this list that it not grow excessively large, consider adding to t</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>. {{fact}}\\n&lt;eos&gt;&lt;bos&gt;[[ms:Autisme]]\\n&lt;eos&gt;&lt;bos&gt;      &lt;/contributor&gt;\\n&lt;eos&gt;&lt;bos&gt;    &lt;title&gt;AlchemY&lt;/title&gt;\\n&lt;eos&gt;&lt;bos&gt;      &lt;text xml:space=\"preserve\"&gt;#REDIRECT [[A</td>\n",
              "      <td>{{fact}}\\n&lt;eos&gt;&lt;bos&gt;[[ms:Autisme]]\\n&lt;eos&gt;&lt;bos&gt;      &lt;/contributor&gt;\\n&lt;eos&gt;&lt;bos&gt;    &lt;title&gt;AlchemY&lt;/title&gt;\\n&lt;eos&gt;&lt;bos&gt;      &lt;text xml:space=\"preserve\"&gt;#REDIRECT [[Ac</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yfhrOFtrHkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8758f926-c034-47e9-93bb-f093462c3d8d"
      },
      "source": [
        "xb, yb = dls.one_batch()\n",
        "xb.shape, yb.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 128), (32, 128))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmiyNrZCrHkd"
      },
      "source": [
        "vocab_sz = bte.vocab_size"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAHrj8xsrHkd"
      },
      "source": [
        "# Begin Experiment Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCfc4LIbycYK"
      },
      "source": [
        "from fastai.callback.wandb import *"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRGplPrYzJob",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2926424c-7f83-44c7-9ccd-5048672c3861"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfastai_community\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tigjgHsVzJmT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "ca6646b0-f9af-413c-9593-846cefee41cf"
      },
      "source": [
        "wandb.init(reinit=True, project=\"reformer-fastai\", name=WANDB_NAME, group=GROUP, notes=NOTES,  tags=TAGS)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.11<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">enc_lm_enwik8</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/fastai_community/reformer-fastai\" target=\"_blank\">https://wandb.ai/fastai_community/reformer-fastai</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/fastai_community/reformer-fastai/runs/3h3hhb3n\" target=\"_blank\">https://wandb.ai/fastai_community/reformer-fastai/runs/3h3hhb3n</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20201127_202505-3h3hhb3n</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f55e604c2b0>"
            ],
            "text/html": [
              "<h1>Run(3h3hhb3n)</h1><p></p><iframe src=\"https://wandb.ai/fastai_community/reformer-fastai/runs/3h3hhb3n\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzaKSXYXrHkd"
      },
      "source": [
        "learn = Learner(dls, TransformerLM(vocab_sz, 512),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=[accuracy, Perplexity()])#.to_native_fp16()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg8IMkTVrHkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "1fb17013-c7d1-4fe3-d2f3-4dd1bda137ab"
      },
      "source": [
        "learn.lr_find()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.0691830962896347)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLVZVGflrHkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "ca178c8d-f622-41fb-ba91-620382abe44c"
      },
      "source": [
        "learn.fit_one_cycle(2, 5e-4, wd=0.05,cbs=WandbCallback(log_model=False))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.434971</td>\n",
              "      <td>3.353122</td>\n",
              "      <td>0.177170</td>\n",
              "      <td>28.591852</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.463178</td>\n",
              "      <td>3.110328</td>\n",
              "      <td>0.220311</td>\n",
              "      <td>22.428394</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WandbCallback was not able to get prediction samples -> wandb.log must be passed a dictionary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2XCNjZvYclQ"
      },
      "source": [
        "class Adafactor(torch.optim.Optimizer):\n",
        "    \"\"\"Implements Adafactor algorithm.\n",
        "\n",
        "    This implementation is based on:\n",
        "    `Adafactor: Adaptive Learning Rates with Sublinear Memory Cost`\n",
        "    (see https://arxiv.org/abs/1804.04235)\n",
        "\n",
        "    Note that this optimizer internally adjusts the learning rate\n",
        "    depending on the *scale_parameter*, *relative_step* and\n",
        "    *warmup_init* options. To use a manual (external) learning rate\n",
        "    schedule you should set `scale_parameter=False` and\n",
        "    `relative_step=False`.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): external learning rate (default: None)\n",
        "        eps (tuple[float, float]): regularization constans for square gradient\n",
        "            and parameter scale respectively (default: (1e-30, 1e-3))\n",
        "        clip_threshold (float): threshold of root mean square of\n",
        "            final gradient update (default: 1.0)\n",
        "        decay_rate (float): coefficient used to compute running averages of square\n",
        "            gradient (default: -0.8)\n",
        "        mom (float): coefficient used for computing running averages of gradient\n",
        "            (default: None)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        scale_parameter (bool): if True, learning rate is scaled by root mean square of\n",
        "            parameter (default: True)\n",
        "        relative_step (bool): if True, time-dependent learning rate is computed\n",
        "            instead of external learning rate (default: True)\n",
        "        warmup_init (bool): time-dependent learning rate computation depends on\n",
        "            whether warm-up initialization is being used (default: False)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,\n",
        "        lr=None,\n",
        "        eps=(1e-30, 1e-3),\n",
        "        clip_threshold=1.0,\n",
        "        decay_rate=-0.8,\n",
        "        mom=None,\n",
        "        weight_decay=0.0,\n",
        "        scale_parameter=True,\n",
        "        relative_step=True,\n",
        "        warmup_init=False,\n",
        "    ):\n",
        "        if lr is not None and relative_step:\n",
        "            raise ValueError(\"Cannot combine manual lr and relative_step options\")\n",
        "        if warmup_init and not relative_step:\n",
        "            raise ValueError(\"warmup_init requires relative_step=True\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr,\n",
        "            eps=eps,\n",
        "            clip_threshold=clip_threshold,\n",
        "            decay_rate=decay_rate,\n",
        "            mom=mom,\n",
        "            weight_decay=weight_decay,\n",
        "            scale_parameter=scale_parameter,\n",
        "            relative_step=relative_step,\n",
        "            warmup_init=warmup_init,\n",
        "        )\n",
        "        super(Adafactor, self).__init__(params, defaults)\n",
        "\n",
        "    @property\n",
        "    def supports_memory_efficient_fp16(self):\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def supports_flat_params(self):\n",
        "        return False\n",
        "\n",
        "    def _get_lr(self, param_group, param_state):\n",
        "        rel_step_sz = param_group[\"lr\"]\n",
        "        if param_group[\"relative_step\"]:\n",
        "            min_step = (\n",
        "                1e-6 * param_state[\"step\"] if param_group[\"warmup_init\"] else 1e-2\n",
        "            )\n",
        "            rel_step_sz = min(min_step, 1.0 / math.sqrt(param_state[\"step\"]))\n",
        "        param_scale = 1.0\n",
        "        if param_group[\"scale_parameter\"]:\n",
        "            param_scale = max(param_group[\"eps\"][1], param_state[\"RMS\"])\n",
        "        return param_scale * rel_step_sz\n",
        "\n",
        "    def _get_options(self, param_group, param_shape):\n",
        "        factored = len(param_shape) >= 2\n",
        "        use_first_moment = param_group[\"mom\"] is not None\n",
        "        return factored, use_first_moment\n",
        "\n",
        "    def _rms(self, tensor):\n",
        "        return tensor.norm(2) / (tensor.numel() ** 0.5)\n",
        "\n",
        "    def _approx_sq_grad(self, exp_avg_sq_row, exp_avg_sq_col):\n",
        "        r_factor = (\n",
        "            (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True))\n",
        "            .rsqrt_()\n",
        "            .unsqueeze(-1)\n",
        "        )\n",
        "        c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()\n",
        "        return torch.mul(r_factor, c_factor)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    grad = grad.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adafactor does not support sparse gradients.\")\n",
        "\n",
        "                state = self.state[p]\n",
        "                grad_shape = grad.shape\n",
        "\n",
        "                factored, use_first_moment = self._get_options(group, grad_shape)\n",
        "                # State Initialization\n",
        "                if len(state) == 0:\n",
        "                    state[\"step\"] = 0\n",
        "\n",
        "                    if use_first_moment:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state[\"exp_avg\"] = torch.zeros_like(grad)\n",
        "                    if factored:\n",
        "                        state[\"exp_avg_sq_row\"] = torch.zeros(grad_shape[:-1]).to(grad)\n",
        "                        state[\"exp_avg_sq_col\"] = torch.zeros(\n",
        "                            grad_shape[:-2] + grad_shape[-1:]\n",
        "                        ).to(grad)\n",
        "                    else:\n",
        "                        state[\"exp_avg_sq\"] = torch.zeros_like(grad)\n",
        "\n",
        "                    state[\"RMS\"] = 0\n",
        "                else:\n",
        "                    if use_first_moment:\n",
        "                        state[\"exp_avg\"] = state[\"exp_avg\"].to(grad)\n",
        "                    if factored:\n",
        "                        state[\"exp_avg_sq_row\"] = state[\"exp_avg_sq_row\"].to(grad)\n",
        "                        state[\"exp_avg_sq_col\"] = state[\"exp_avg_sq_col\"].to(grad)\n",
        "                    else:\n",
        "                        state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].to(grad)\n",
        "\n",
        "                p_data_fp32 = p.data\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p_data_fp32 = p_data_fp32.float()\n",
        "\n",
        "                state[\"step\"] += 1\n",
        "                state[\"RMS\"] = self._rms(p_data_fp32)\n",
        "                group[\"lr\"] = self._get_lr(group, state)\n",
        "\n",
        "                beta2t = 1.0 - math.pow(state[\"step\"], group[\"decay_rate\"])\n",
        "                update = (grad ** 2) + group[\"eps\"][0]\n",
        "                if factored:\n",
        "                    exp_avg_sq_row = state[\"exp_avg_sq_row\"]\n",
        "                    exp_avg_sq_col = state[\"exp_avg_sq_col\"]\n",
        "\n",
        "                    exp_avg_sq_row.mul_(beta2t).add_(\n",
        "                        update.mean(dim=-1), alpha=1.0 - beta2t\n",
        "                    )\n",
        "                    exp_avg_sq_col.mul_(beta2t).add_(\n",
        "                        update.mean(dim=-2), alpha=1.0 - beta2t\n",
        "                    )\n",
        "\n",
        "                    # Approximation of exponential moving average of square of gradient\n",
        "                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)\n",
        "                    update.mul_(grad)\n",
        "                else:\n",
        "                    exp_avg_sq = state[\"exp_avg_sq\"]\n",
        "\n",
        "                    exp_avg_sq.mul_(beta2t).add_(update, alpha=1.0 - beta2t)\n",
        "                    update = exp_avg_sq.rsqrt().mul_(grad)\n",
        "\n",
        "                update.div_(\n",
        "                    (self._rms(update) / group[\"clip_threshold\"]).clamp_(min=1.0)\n",
        "                )\n",
        "                update.mul_(group[\"lr\"])\n",
        "\n",
        "                if use_first_moment:\n",
        "                    exp_avg = state[\"exp_avg\"]\n",
        "                    exp_avg.mul_(group[\"mom\"]).add_(update, alpha=1 - group[\"mom\"])\n",
        "                    update = exp_avg\n",
        "\n",
        "                if group[\"weight_decay\"] != 0:\n",
        "                    p_data_fp32.add_(\n",
        "                        p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"]\n",
        "                    )\n",
        "\n",
        "                p_data_fp32.add_(-update)\n",
        "\n",
        "                if p.data.dtype in {torch.float16, torch.bfloat16}:\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hciLYzYYf0k"
      },
      "source": [
        "@delegates(Adafactor.__init__)\n",
        "def pytorch_Adafactor(param_groups, **kwargs):\n",
        "    return OptimWrapper(Adafactor([{'params': ps, **kwargs} for ps in param_groups]))\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8WwoGmbYg2K",
        "outputId": "ced30e13-ea22-4e6e-d186-cfd4c8e03eef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "learn = Learner(dls, TransformerLM(vocab_sz, 512),\n",
        "                loss_func=CrossEntropyLossFlat(),\n",
        "                metrics=[accuracy, Perplexity()])#.to_native_fp16()\n",
        "learn.opt_func=partial(pytorch_Adafactor, weight_decay=0.0)\n",
        "learn.opt_func"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "functools.partial(<function pytorch_Adafactor at 0x7f55847c2048>, weight_decay=0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B_FB3spYiVS",
        "outputId": "6f351d6a-09d2-4413-a42b-7a5eb98d25ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "learn.fit_one_cycle(2, 5e-4,cbs=WandbCallback(log_model=False))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.051103</td>\n",
              "      <td>3.363344</td>\n",
              "      <td>0.209741</td>\n",
              "      <td>28.885628</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.300203</td>\n",
              "      <td>3.008500</td>\n",
              "      <td>0.234016</td>\n",
              "      <td>20.256989</td>\n",
              "      <td>00:13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WandbCallback was not able to get prediction samples -> wandb.log must be passed a dictionary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B-hR_XpYm8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}