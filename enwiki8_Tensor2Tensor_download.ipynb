{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "enwiki8 Tensor2Tensor download",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morganmcg1/reformer-fastai/blob/main/enwiki8_Tensor2Tensor_download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odi2vIMHC3Rm"
      },
      "source": [
        "# enwik8 in Tensor2Tensor\n",
        "Code to download and process enwik8 dataset, as used in the Reformer paper\n",
        "\n",
        "This code based on the [Tensor2Tensor introduction notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb#scrollTo=RrlewY6Lu68v)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGni6fuvoTj",
        "outputId": "81c9bee6-b9f3-46b5-8f30-3a2ca487d321",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install deps\n",
        "!pip install -q -U tensor2tensor\n",
        "!pip install -q tensorflow matplotlib"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5MB 4.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 23.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 29.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 45.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 19.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 38.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 51.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 41.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 52.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.5MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GTATLJR9MIm"
      },
      "source": [
        "**NOTE**: if `from tensor2tensor import problems` throws an error, just run the cell again and it should work fine "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oILRLCWN_16u"
      },
      "source": [
        "# Imports we need.\n",
        "import sys\n",
        "if 'google.colab' in sys.modules: # Colab-only TensorFlow version selector\n",
        "  %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "\n",
        "from tensor2tensor import problems\n",
        "\n",
        "# Enable TF Eager execution\n",
        "tfe = tf.contrib.eager\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "# Other setup\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "# # Setup some directories\n",
        "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
        "tmp_dir = os.path.expanduser(\"~/t2t/tmp\")\n",
        "# train_dir = os.path.expanduser(\"~/t2t/train\")\n",
        "# checkpoint_dir = os.path.expanduser(\"~/t2t/checkpoints\")\n",
        "tf.gfile.MakeDirs(data_dir)\n",
        "tf.gfile.MakeDirs(tmp_dir)\n",
        "# tf.gfile.MakeDirs(train_dir)\n",
        "# tf.gfile.MakeDirs(checkpoint_dir)\n",
        "# gs_data_dir = \"gs://tensor2tensor-data\"\n",
        "# gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONjOtKwa9yjY"
      },
      "source": [
        "See the 2 folder created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DynbM7G59ysP"
      },
      "source": [
        "!ls ../root/t2t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a69r1KDiZDe"
      },
      "source": [
        "# Download enwik8 and inspect it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LIxRABX9YZx"
      },
      "source": [
        "A Problem is a dataset together with some fixed pre-processing. It could be a translation dataset with a specific tokenization, or an image dataset with a specific resolution. There are many problems available in Tensor2Tensor.\n",
        "\n",
        "## enwik8 Problem\n",
        "The data defined in the [Trax Reformer enwiki8 config](https://github.com/google/trax/blob/f8024e8057599b92fce82842f342cb3d39c8f405/trax/supervised/configs/reformer_enwik8.gin) uses the `t2t_enwik8_l65k` Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYDMO4zArgkz"
      },
      "source": [
        "# UNCOMMENT THE BELOW TO SEE ALL AVAILABLE PROBLEMS\n",
        "#problems.available()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlewY6Lu68v",
        "outputId": "4f071c64-e678-48cb-e7db-25e1d4e1266d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "enwik8_l65k = problems.problem(\"enwik8_l65k\")\n",
        "\n",
        "# The generate_data method of a problem will download data and process it into\n",
        "# a standard format ready for training and evaluation.\n",
        "enwik8_l65k.generate_data(data_dir, tmp_dir)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:134: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:134: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:168: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:168: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:236: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:236: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Downloading http://mattmahoney.net/dc/enwik8.zip to /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Downloading http://mattmahoney.net/dc/enwik8.zip to /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:238: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:238: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "100% completed\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:246: The name tf.gfile.Rename is deprecated. Please use tf.io.gfile.rename instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:246: The name tf.gfile.Rename is deprecated. Please use tf.io.gfile.rename instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Successfully downloaded enwik8.zip, 36445475 bytes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Successfully downloaded enwik8.zip, 36445475 bytes.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'train' = 89621832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'train' = 89621832\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 5471 Examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 5471 Examples\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'eval' = 5000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'eval' = 5000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 77 Examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 77 Examples\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /root/t2t/tmp/enwik8.zip\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of enwik8 = 99621832\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'test' = 5000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Length of split 'test' = 5000000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generating case 0.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 77 Examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Generated 77 Examples\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Shuffling data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Shuffling data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:473: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:473: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:517: The name tf.gfile.Remove is deprecated. Please use tf.io.gfile.remove instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensor2tensor/data_generators/generator_utils.py:517: The name tf.gfile.Remove is deprecated. Please use tf.io.gfile.remove instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Data shuffled.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Data shuffled.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpA3vaxLvtik"
      },
      "source": [
        "#!head ../root/t2t/data/enwik8_l65k-train-00005-of-00016"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_dxWR0499Tg"
      },
      "source": [
        "Inspect the raw file enwik8 txt file downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug68UIKI3Nm7",
        "outputId": "f71cb287-3aab-48ef-e238-ba165dbcb46e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!head -n 50 ../root/t2t/tmp/enwik8"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\n",
            "  <siteinfo>\n",
            "    <sitename>Wikipedia</sitename>\n",
            "    <base>http://en.wikipedia.org/wiki/Main_Page</base>\n",
            "    <generator>MediaWiki 1.6alpha</generator>\n",
            "    <case>first-letter</case>\n",
            "      <namespaces>\n",
            "      <namespace key=\"-2\">Media</namespace>\n",
            "      <namespace key=\"-1\">Special</namespace>\n",
            "      <namespace key=\"0\" />\n",
            "      <namespace key=\"1\">Talk</namespace>\n",
            "      <namespace key=\"2\">User</namespace>\n",
            "      <namespace key=\"3\">User talk</namespace>\n",
            "      <namespace key=\"4\">Wikipedia</namespace>\n",
            "      <namespace key=\"5\">Wikipedia talk</namespace>\n",
            "      <namespace key=\"6\">Image</namespace>\n",
            "      <namespace key=\"7\">Image talk</namespace>\n",
            "      <namespace key=\"8\">MediaWiki</namespace>\n",
            "      <namespace key=\"9\">MediaWiki talk</namespace>\n",
            "      <namespace key=\"10\">Template</namespace>\n",
            "      <namespace key=\"11\">Template talk</namespace>\n",
            "      <namespace key=\"12\">Help</namespace>\n",
            "      <namespace key=\"13\">Help talk</namespace>\n",
            "      <namespace key=\"14\">Category</namespace>\n",
            "      <namespace key=\"15\">Category talk</namespace>\n",
            "      <namespace key=\"100\">Portal</namespace>\n",
            "      <namespace key=\"101\">Portal talk</namespace>\n",
            "    </namespaces>\n",
            "  </siteinfo>\n",
            "  <page>\n",
            "    <title>AaA</title>\n",
            "    <id>1</id>\n",
            "    <revision>\n",
            "      <id>32899315</id>\n",
            "      <timestamp>2005-12-27T18:46:47Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Jsmethers</username>\n",
            "        <id>614213</id>\n",
            "      </contributor>\n",
            "      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\n",
            "    </revision>\n",
            "  </page>\n",
            "  <page>\n",
            "    <title>AlgeriA</title>\n",
            "    <id>5</id>\n",
            "    <revision>\n",
            "      <id>18063769</id>\n",
            "      <timestamp>2005-07-03T11:13:13Z</timestamp>\n",
            "      <contributor>\n",
            "        <username>Docu</username>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5SESEfI-dRc"
      },
      "source": [
        "## Vocab Type\n",
        "\n",
        "This will tell us how the dataset is encoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Oqg-z20-OqK",
        "outputId": "c92117db-9c60-450d-b291-4f447812482d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# `CHARACTER`: `ByteTextEncoder`, encode raw bytes.\n",
        "enwik8_l65k.vocab_type"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'character'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m363n9-Z-uWj"
      },
      "source": [
        "# Dataset is already encoded\n",
        "\n",
        "Tensor2Tensor problems include preprocessing, in this case enwik8 is encoded with a `ByteTextEncoder`, defined below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p45jCWtn-uee",
        "outputId": "25d0a700-a3c8-4131-c991-531731e81c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Now let's see the training MNIST data as Tensors.\n",
        "enwik8_example = tfe.Iterator(enwik8_l65k.dataset(Modes.TRAIN, data_dir)).next()\n",
        "\n",
        "print()\n",
        "#print(enwik8_example)\n",
        "# print()\n",
        "print(enwik8_example['targets'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /root/t2t/data/enwik8_l65k-train*\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /root/t2t/data/enwik8_l65k-train*\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:partition: 0 num_data_files: 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:partition: 0 num_data_files: 16\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function Problem.dataset.<locals>._load_records_and_preprocess at 0x7fbbacda9620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <function Problem.dataset.<locals>._load_records_and_preprocess at 0x7fbbacda9620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <function Problem.dataset.<locals>._load_records_and_preprocess at 0x7fbbacda9620> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Problem.maybe_reverse_and_copy of <tensor2tensor.data_generators.enwik8.Enwik8L65k object at 0x7fbbbda56a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Entity <bound method Problem.maybe_reverse_and_copy of <tensor2tensor.data_generators.enwik8.Enwik8L65k object at 0x7fbbbda56a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Entity <bound method Problem.maybe_reverse_and_copy of <tensor2tensor.data_generators.enwik8.Enwik8L65k object at 0x7fbbbda56a90>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "\n",
            "{'batch_prediction_key': <tf.Tensor: id=825, shape=(1,), dtype=int64, numpy=array([0])>, 'targets': <tf.Tensor: id=826, shape=(65658,), dtype=int64, numpy=array([105, 118,  61, ..., 100,  99, 118])>}\n",
            "tf.Tensor([105 118  61 ... 100  99 118], shape=(65658,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y58wnph84PoL"
      },
      "source": [
        "\n",
        "# Tracking down the encoding used:\n",
        "\n",
        "## Trax `tf_inputs`:\n",
        "In the Reformer enwik8 config `tf_inputs` is called to handle the data streams\n",
        "\n",
        "tf_inputs:\n",
        "https://github.com/google/trax/blob/f8024e8057599b92fce82842f342cb3d39c8f405/trax/data/tf_inputs.py#L265\n",
        "\n",
        "Within `tf_inputs`, `_train_and_eval_dataset_v1` is used to call the `t2t_enwik8_l65k` dataset (\"t2t_enwik8_l65k\" is named in the Reformer config)\n",
        "\n",
        "## `Enwik8L65k` dataset\n",
        "The `t2t_enwik8_l65k` string maps to the `Enwik8L65k` dataset Tensor2Tensor \"Problem\" (https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/enwik8.py). \n",
        "\n",
        "`Enwik8L65k` subclasses `Text2SelfProblem` which subclasses `Text2TextProblem` in `text_problems.py`.\n",
        "\n",
        "## `Text2TextProblem` Problem class\n",
        "\n",
        "`Text2TextProblem` calls `text_encoder.ByteTextEncoder()`\n",
        "\n",
        "`Text2TextProblem` is defined in `text_problems.py`: https://github.com/tensorflow/tensor2tensor/blob/5f9dd2db6d7797162e53adf152310ed13e9fc711/tensor2tensor/data_generators/text_problems.py#L53\n",
        "\n",
        "\n",
        "\n",
        "## `ByteTextEncoder` Text Encoder\n",
        "\n",
        "`ByteTextEncoder` is defined in `text_encoder.py`\n",
        "\n",
        "https://github.com/tensorflow/tensor2tensor/blob/5f9dd2db6d7797162e53adf152310ed13e9fc711/tensor2tensor/data_generators/text_encoder.py#L176\n",
        "\n",
        "`ByteTextEncoder` defined as:\n",
        "\n",
        "```\n",
        "class ByteTextEncoder(TextEncoder):\n",
        "  \"\"\"Encodes each byte to an id. For 8-bit strings only.\"\"\"\n",
        "\n",
        "  def encode(self, s):\n",
        "    numres = self._num_reserved_ids\n",
        "    if six.PY2:\n",
        "      if isinstance(s, unicode):\n",
        "        s = s.encode(\"utf-8\")\n",
        "      return [ord(c) + numres for c in s]\n",
        "    # Python3: explicitly convert to UTF-8\n",
        "    return [c + numres for c in s.encode(\"utf-8\")]\n",
        "\n",
        "  def decode(self, ids, strip_extraneous=False):\n",
        "    if strip_extraneous:\n",
        "      ids = strip_ids(ids, list(range(self._num_reserved_ids or 0)))\n",
        "    numres = self._num_reserved_ids\n",
        "    decoded_ids = []\n",
        "    int2byte = six.int2byte\n",
        "    for id_ in ids:\n",
        "      if 0 <= id_ < numres:\n",
        "        decoded_ids.append(RESERVED_TOKENS_BYTES[int(id_)])\n",
        "      else:\n",
        "        decoded_ids.append(int2byte(id_ - numres))\n",
        "    if six.PY2:\n",
        "      return \"\".join(decoded_ids)\n",
        "    # Python3: join byte arrays and then decode string\n",
        "    return b\"\".join(decoded_ids).decode(\"utf-8\", \"replace\")\n",
        "\n",
        "  def decode_list(self, ids):\n",
        "    numres = self._num_reserved_ids\n",
        "    decoded_ids = []\n",
        "    int2byte = six.int2byte\n",
        "    for id_ in ids:\n",
        "      if 0 <= id_ < numres:\n",
        "        decoded_ids.append(RESERVED_TOKENS_BYTES[int(id_)])\n",
        "      else:\n",
        "        decoded_ids.append(int2byte(id_ - numres))\n",
        "    # Python3: join byte arrays and then decode string\n",
        "    return decoded_ids\n",
        "\n",
        "  @property\n",
        "  def vocab_size(self):\n",
        "    return 2**8 + self._num_reserved_ids\n",
        "\n",
        "```"
      ]
    }
  ]
}